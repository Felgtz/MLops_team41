#!/usr/bin/env python
"""
scripts/evaluate.py
===================

Compute MAE, RMSE and R² between a set of ground-truth values and the
predictions generated by `scripts/predict.py` (or any other source).

Scenarios covered
-----------------
1. 1-to-1 alignment by *row order*  
   • Simply pass two equal-length CSV / JSON files.

2. Alignment on a unique *identifier* column (e.g. "id")  
   • Provide the column name via `--id-col`; the script performs an inner join
     before scoring.

Outputs
-------
• Prints the metrics to stdout.  
• Optionally writes them to a JSON file (`--output metrics.json`).  
• Returns zero exit status only if the join succeeds and metrics are computed.

Example
-------
python scripts/evaluate.py \
    --truth data/processed/test_labels.csv --target-col price \
    --pred predictions.csv \
    --id-col id \
    --output artifacts/metrics/offline_eval.json
"""

from __future__ import annotations

import argparse
import json
import logging
from pathlib import Path
from typing import Tuple

import pandas as pd

from src.models.evaluate import eval_regression
from src.utils import ensure_dir, save_json

logger = logging.getLogger(__name__)


# --------------------------------------------------------------------------- #
# 1. Helpers                                                                  #
# --------------------------------------------------------------------------- #
def _load_file(path: Path) -> pd.DataFrame | pd.Series:
    if path.suffix.lower() == ".csv":
        return pd.read_csv(path)
    if path.suffix.lower() == ".json":
        with path.open() as f:
            data = json.load(f)
        # list/array → Series; dict or list-of-dicts → DataFrame
        return pd.Series(data) if isinstance(data, list) else pd.DataFrame(data)
    raise ValueError("Only .csv or .json files are supported.")


def _align_truth_preds(
    truth_df: pd.DataFrame,
    preds_df: pd.DataFrame,
    id_col: str | None,
    target_col: str,
) -> Tuple[pd.Series, pd.Series]:
    if id_col:  # join on id
        for df, name in [(truth_df, "truth"), (preds_df, "preds")]:
            if id_col not in df.columns:
                raise KeyError(f"Column '{id_col}' not found in {name} file.")

        joined = truth_df[[id_col, target_col]].merge(
            preds_df[[id_col, "prediction"]], on=id_col, how="inner", validate="one_to_one"
        )
        if joined.empty:
            raise ValueError("Join produced 0 rows—check your id column.")
        return joined[target_col], joined["prediction"]

    # Else rely on positional alignment
    if len(truth_df) != len(preds_df):
        raise ValueError(
            "Files have different number of rows and no --id-col supplied for alignment."
        )
    # Retrieve Series
    y_true = truth_df[target_col] if target_col in truth_df else truth_df.squeeze()
    y_pred = preds_df["prediction"] if "prediction" in preds_df else preds_df.squeeze()
    return y_true, y_pred


def _pretty(metrics: Tuple[float, float, float]) -> str:
    mae, rmse, r2 = metrics
    return f"MAE: {mae:.2f} | RMSE: {rmse:.2f} | R²: {r2:.4f}"


# --------------------------------------------------------------------------- #
# 2. CLI                                                                      #
# --------------------------------------------------------------------------- #
def _parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Evaluate predictions against ground truth.")
    p.add_argument(
        "--truth",
        "-y",
        required=True,
        type=str,
        help="CSV/JSON with the ground-truth target (and optional id column).",
    )
    p.add_argument(
        "--pred",
        "-p",
        required=True,
        type=str,
        help="CSV/JSON with model predictions (column 'prediction' expected if CSV).",
    )
    p.add_argument(
        "--target-col",
        "-t",
        default="target",
        help="Column name of the ground-truth values in --truth (default: 'target').",
    )
    p.add_argument(
        "--id-col",
        "-i",
        default=None,
        help="Optional column on which to join truth and preds before scoring.",
    )
    p.add_argument(
        "--output",
        "-o",
        type=str,
        default=None,
        help="If set, write metrics JSON to this path.",
    )
    return p.parse_args()


def main() -> None:
    args = _parse_args()

    truth_path = Path(args.truth)
    pred_path = Path(args.pred)

    truth_df = _load_file(truth_path)
    preds_df = _load_file(pred_path)

    y_true, y_pred = _align_truth_preds(
        truth_df, preds_df, id_col=args.id_col, target_col=args.target_col
    )

    mae, rmse, r2 = eval_regression(y_true=y_true.values, y_pred_log=np.log1p(y_pred))

    # Print to console
    msg = _pretty((mae, rmse, r2))
    print("\n" + "=" * 28 + "\nOffline evaluation\n" + "=" * 28)
    print(msg)

    # Optional persistence
    if args.output:
        out_path = ensure_dir(Path(args.output))
        metrics = {"MAE": mae, "RMSE": rmse, "R2": r2}
        save_json(metrics, out_path)
        logger.info("Saved metrics to %s", out_path)


# --------------------------------------------------------------------------- #
# 3. Entry-point guard                                                        #
# --------------------------------------------------------------------------- #
if __name__ == "__main__":
    import numpy as np  # local import so library users aren’t forced to have NumPy

    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s"
    )
    main()
