Metadata-Version: 2.4
Name: mlops_MCE
Version: 0.1.0
Summary: End-to-end tabular ML pipeline (Team 41)
Author-email: Team 41 <team41@example.com>
License: MIT
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: pandas>=1.5
Requires-Dist: numpy>=1.22
Requires-Dist: scikit-learn>=1.2
Requires-Dist: joblib>=1.2
Requires-Dist: pyyaml>=6.0
Requires-Dist: fsspec>=2023.4.0
Provides-Extra: xgboost
Requires-Dist: xgboost>=1.7; extra == "xgboost"
Provides-Extra: viz
Requires-Dist: matplotlib>=3.7; extra == "viz"
Requires-Dist: seaborn>=0.13; extra == "viz"
Provides-Extra: dev
Requires-Dist: pytest>=7.4; extra == "dev"
Requires-Dist: black>=23.9; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Requires-Dist: mypy>=1.5; extra == "dev"
Requires-Dist: build>=0.10; extra == "dev"
Requires-Dist: twine>=4.0; extra == "dev"

# ğŸ—ï¸  Tabular ML Boilerplate

**One-command** end-to-end pipeline for tabular (regression or classification)
projects:

1. Load & split a simple CSV  
2. Perform light feature-engineering  
3. Train a baseline or boosted model  
4. Persist artefacts + metrics  
5. Run batch inference

> Built with plain `pandas` + `scikit-learn` and optionally `xgboost`.  
> Everything is unit-tested, type-hinted, and PEP 517-compliant.

---

## ğŸ“ Directory layout
. â”œâ”€â”€ src/ â”‚ â”œâ”€â”€ config/ â”‚ â”‚ â””â”€â”€ experiment.yaml # declarative experiment file â”‚ â”œâ”€â”€ data/ â”‚ â”‚ â”œâ”€â”€ init.py â”‚ â”‚ â””â”€â”€ load_and_split.py â”‚ â”œâ”€â”€ features/ â”‚ â”‚ â”œâ”€â”€ init.py â”‚ â”‚ â”œâ”€â”€ drop_columns.py â”‚ â”‚ â””â”€â”€ drop_high_corr_features.py â”‚ â”œâ”€â”€ models/ â”‚ â”‚ â”œâ”€â”€ init.py â”‚ â”‚ â”œâ”€â”€ baseline.py â”‚ â”‚ â”œâ”€â”€ boosting.py â”‚ â”‚ â”œâ”€â”€ train.py â”‚ â”‚ â””â”€â”€ predict.py â”‚ â””â”€â”€ pipelines/ â”‚ â”œâ”€â”€ init.py â”‚ â””â”€â”€ training_pipeline.py # (stub - extend as you like) â”œâ”€â”€ scripts/ â”‚ â”œâ”€â”€ train.py # CLI: train --config â€¦ â”‚ â””â”€â”€ predict.py # CLI: predict --input â€¦ --model â€¦ â”œâ”€â”€ tests/ # pytest unit tests â””â”€â”€ pyproject.toml



---

## ğŸš€ Quick-start

### 0) Clone & install

```bash
git clone https://github.com/your-org/your-repo.git
cd your-repo

# core deps
pip install -e .

# optional extra for XGBoost
pip install -e .[xgboost]
1) Prepare your data
Put a single CSV under data/raw/, e.g.


data/raw/housing.csv
The file must contain the target column you specify in the config (price in the example).

2) Configure the experiment
Edit src/config/experiment.yaml:

yaml

data:
  path: "data/raw/housing.csv"
  target_column: "price"
  test_size: 0.2
  random_state: 42
preprocessing:
  drop_columns: ["id", "zipcode"]
  drop_high_corr:
    threshold: 0.95
model:
  type: "xgboost"            # or "baseline"
  params:
    n_estimators: 500
training:
  cv_folds: 5
output:
  artifacts_dir: "artifacts"
3) Train
bash

python scripts/train.py --config src/config/experiment.yaml
Outputs:


artifacts/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ xgboost.pkl
â””â”€â”€ metrics/
    â””â”€â”€ xgboost_metrics.json
4) Predict
bash

python scripts/predict.py \
  --input data/raw/housing.csv \
  --output predictions.csv    \
  --model xgboost
ğŸ§° Library API
python

from src.data import load_csv, train_test_split_df
from src.features import DropColumns, DropHighCorrFeatures
from src.models.train import train_model
from src.models.predict import run_prediction
Module	Key functions/classes
src.data	load_csv, train_test_split_df
src.features	DropColumns, DropHighCorrFeatures
src.models.baseline	BaselineConfig, train_and_evaluate()
src.models.boosting	BoostingConfig, train_and_evaluate()
src.models.train	train_model() â€“ unified faÃ§ade
src.models.predict	run_prediction(), list_available_models()
ğŸ§ª Testing & linting
bash

# run unit tests
pytest -q

# static analysis
ruff check .
mypy src/ tests/
âœ¨ Extending
New feature transformer
Add src/features/my_transformer.py inheriting BaseEstimator + TransformerMixin, then export it in src/features/__init__.py.

New model back-end

Implement src/models/lightgbm.py with a dataclass LightGBMConfig and train_and_evaluate() routine.

Register it in src/models/train.py:

python

from .lightgbm import LightGBMConfig, train_and_evaluate as _run_lgbm
_REGISTRY["lightgbm"] = (_run_lgbm, LightGBMConfig)
More complex data sources
Swap load_and_split.py for a reader that pulls from SQL, BigQuery, or a feature storeâ€”the rest of the pipeline stays unchanged.

ğŸ¤ Contributing
PRs are very welcome! Please:

Open an issue to discuss major changes first.
Create a feature branch (git checkout -b feat/my-feature).
Ensure pytest, ruff, and mypy pass.
Submit a pull request describing why the change is needed.
ğŸ“„ License
MIT â€” see LICENSE for details.


