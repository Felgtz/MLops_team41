{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "2AKmE7AIb7Zh",
    "papermill": {
     "duration": 0.007056,
     "end_time": "2025-10-12T21:17:06.390797",
     "exception": false,
     "start_time": "2025-10-12T21:17:06.383741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Phase 1 – Exploratory Data Analysis and Data Preparation**\n",
    "### *MLOps Course Project: Online News Popularity Dataset*\n",
    "**Team:** *41*  \n",
    "**Course:** *Machine Learning Operations (MLOps)*  \n",
    "**Institution:** Tecnológico de Monterrey  \n",
    "**Date:** *October 7th, 2025*  \n",
    "\n",
    "---\n",
    "\n",
    "### **Team Members & Roles**\n",
    "- **Data Engineer:** *Angel Iván Ahumada Arguelles*  \n",
    "- **Data Scientist:** *Steven Sebastian Brutscher Cortez*  \n",
    "- **Software Engineer:** *Ana Karen Estupiñán Pacheco*  \n",
    "- **ML Engineer:** *Felipe de Jesús Gutiérrez Dávila*  \n",
    "\n",
    "---\n",
    "\n",
    "> *This notebook corresponds to the first collaborative deliverable of the MLOps course project.  \n",
    "It focuses on understanding, cleaning, and preparing the dataset for subsequent modeling and deployment phases.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "eJbHFOeudFQi",
    "papermill": {
     "duration": 0.005862,
     "end_time": "2025-10-12T21:17:06.403985",
     "exception": false,
     "start_time": "2025-10-12T21:17:06.398123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Introduction**\n",
    "\n",
    "This notebook represents the first practical stage (**Phase 1**) of the MLOps course project, focused on the **Online News Popularity** dataset, a well-known dataset originally collected from **Mashable.com**, a global technology and culture media platform.  \n",
    "It contains more than **39,000 news articles**, each characterized by a diverse set of **61 attributes**, including textual, temporal, and social engagement features. The main goal of the dataset is to **predict the popularity of online news articles**, measured by the **number of shares on social media platforms** such as Facebook, Twitter, Google+, LinkedIn, StumbleUpon, and Pinterest.\n",
    "\n",
    "---\n",
    "\n",
    "## **Objective of this Task**\n",
    "\n",
    "The purpose of this first notebook is to:\n",
    "1. **Understand the structure and characteristics** of the dataset through an **Exploratory Data Analysis (EDA)**.  \n",
    "2. **Detect and handle data quality issues**, such as:\n",
    "   - Missing, null, or inconsistent values.  \n",
    "   - Outliers or logically incorrect entries.  \n",
    "   - Redundant or irrelevant features.  \n",
    "3. **Compare the modified version of the dataset** (provided for this course, with added random noise) against the **original UCI dataset**, evaluating the impact of the corruption process and the effectiveness of our data cleaning strategy.  \n",
    "4. **Prepare the final, cleaned dataset** for the next phase of the project, ensuring its **reproducibility and traceability** using tools like **DVC** and **Git** in later stages.\n",
    "\n",
    "---\n",
    "\n",
    "## **Methodological Context**\n",
    "\n",
    "This notebook corresponds to the **“Data Manipulation and Preparation”** task in **Phase 1 – Processing and Initial Modeling**.  \n",
    "While the main focus here is data understanding and preprocessing, it also sets the foundation for the next phases, where the team will:\n",
    "- Organize the project with a **cookiecutter structure**.  \n",
    "- Track experiments and datasets using **MLflow** and **DVC**.  \n",
    "- Build and deploy predictive models through **FastAPI** and **Docker containers**.  \n",
    "\n",
    "In this first step, our goal is to ensure that the data pipeline begins on a **clean, consistent, and interpretable foundation**, replicating the best practices of professional MLOps workflows.\n",
    "\n",
    "---\n",
    "\n",
    "## **About the Dataset**\n",
    "\n",
    "| Property | Description |\n",
    "|-----------|-------------|\n",
    "| **Name** | Online News Popularity |\n",
    "| **Source** | UCI Machine Learning Repository (Fernandes et al., 2015) |\n",
    "| **Instances** | 39,644 |\n",
    "| **Attributes** | 61 total (58 predictive, 2 non-predictive, 1 target) |\n",
    "| **Target Variable** | `shares` (number of times the article was shared) |\n",
    "| **Objective** | Predict or classify the popularity of a news article |\n",
    "| **Period Covered** | January 2013 – January 2015 |\n",
    "| **Data Type** | Mixed: numerical (continuous/discrete) and categorical |\n",
    "| **Languages Used** | English articles from Mashable.com |\n",
    "\n",
    "---\n",
    "\n",
    "## **Next Steps**\n",
    "\n",
    "> **Important Note:**  \n",
    "> While it might be tempting to compare the modified and original datasets column by column during the cleaning process, that approach would not reflect a realistic scenario.  \n",
    "> In real-world projects, data scientists rarely have access to a “perfect” version of their data. Therefore, in this notebook, the analysis and cleaning will be performed **from scratch** on the *modified* dataset, as if it were our only source of information.  \n",
    "> The *original* dataset will be used **only at the end**, to assess how closely our cleaned and processed version matches the correct structure and statistical distribution.  \n",
    "\n",
    "\n",
    "This notebook will follow a structured and realistic workflow designed to emulate a real-world data science scenario.  \n",
    "Although we have access to both the *original* and *modified* versions of the dataset, our main work will focus **entirely on the modified version**, which intentionally contains noise, inconsistencies, missing values, and illogical entries, in order to simulate the process of dealing with imperfect real-world data.\n",
    "\n",
    "The analysis will proceed as follows:\n",
    "\n",
    "1. **Importing libraries and loading the modified dataset.**  \n",
    "2. **Performing an in-depth Exploratory Data Analysis (EDA)** on the modified dataset to understand its structure, detect anomalies, and identify relationships among variables.  \n",
    "3. **Data cleaning and transformation**, including handling missing, inconsistent, or outlier values, as well as adjusting data types and preparing the dataset for modeling.  \n",
    "4. **Comparing the final cleaned dataset** with the original (reference) version provided by the TAs, to evaluate the accuracy and effectiveness of our cleaning and transformation process.  \n",
    "5. **Summarizing results and documenting insights** for use in the following phases of the MLOps project.\n",
    "\n",
    "---\n",
    "\n",
    "> By the end of this notebook, we aim to obtain a clean and well-documented dataset that can serve as the foundation for model building and version control in future phases of the MLOps pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "papermill": {
     "duration": 0.01178,
     "end_time": "2025-10-12T21:17:06.423169",
     "exception": false,
     "start_time": "2025-10-12T21:17:06.411389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Solo monta Drive si estamos en Colab\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "else:\n",
    "    print(\"No Colab: saltando montaje de Drive.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "papermill": {
     "duration": 2.525414,
     "end_time": "2025-10-12T21:17:08.954651",
     "exception": false,
     "start_time": "2025-10-12T21:17:06.429237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Rutas oficiales del repo\n",
    "RAW_MOD = Path(\"data/raw/online_news_modified.csv\")\n",
    "RAW_ORI = Path(\"data/raw/online_news_original.csv\")\n",
    "\n",
    "# Validación básica\n",
    "if not RAW_MOD.exists():\n",
    "    raise FileNotFoundError(f\"No se encontró el modificado: {RAW_MOD}\")\n",
    "if not RAW_ORI.exists():\n",
    "    print(\"⚠️ Aviso: no se encontró el original, se continuará sin comparativa.\")\n",
    "    HAS_ORIG = False\n",
    "else:\n",
    "    HAS_ORIG = True\n",
    "\n",
    "# Lecturas robustas\n",
    "df_mod = pd.read_csv(RAW_MOD, engine=\"python\")  # sep auto\n",
    "print(\"df_mod:\", df_mod.shape)\n",
    "\n",
    "if HAS_ORIG:\n",
    "    df_orig = pd.read_csv(RAW_ORI, engine=\"python\")\n",
    "    print(\"df_orig:\", df_orig.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "papermill": {
     "duration": 0.026751,
     "end_time": "2025-10-12T21:17:08.988202",
     "exception": false,
     "start_time": "2025-10-12T21:17:08.961451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_raw = df_mod.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1760206635028,
     "user": {
      "displayName": "Felipe de Jesús Gutiérrez Dávila",
      "userId": "01500989835494077702"
     },
     "user_tz": 360
    },
    "id": "2odx061SbJj5",
    "outputId": "9ecaa3ab-ae63-4ed8-82d2-75c0a29aa1a9",
    "papermill": {
     "duration": 1.093975,
     "end_time": "2025-10-12T21:17:10.142765",
     "exception": false,
     "start_time": "2025-10-12T21:17:09.048790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Importación de librerías necesarias\n",
    "# ============================================\n",
    "\n",
    "# Manipulación de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Análisis estadístico y preprocesamiento\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Manejo de advertencias\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración general de pandas y visualización\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "print(\"Librerías importadas correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1760206635752,
     "user": {
      "displayName": "Felipe de Jesús Gutiérrez Dávila",
      "userId": "01500989835494077702"
     },
     "user_tz": 360
    },
    "id": "ZtI_lzP4lDQC",
    "outputId": "1e1b4555-dd39-4227-b2da-7fe43640ae7a",
    "papermill": {
     "duration": 0.011613,
     "end_time": "2025-10-12T21:17:10.162130",
     "exception": false,
     "start_time": "2025-10-12T21:17:10.150517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## ===========================================\n",
    "# Conexión con Google Drive\n",
    "# ============================================\n",
    "\n",
    "#from google.colab import drive\n",
    "\n",
    "# Montar Google Drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "papermill": {
     "duration": 2.46982,
     "end_time": "2025-10-12T21:17:12.638342",
     "exception": false,
     "start_time": "2025-10-12T21:17:10.168522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Carga local desde el repo (sin Colab) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "RAW_MOD = Path(\"data/raw/online_news_modified.csv\")\n",
    "RAW_ORI = Path(\"data/raw/online_news_original.csv\")  # opcional, si lo necesitas para comparar\n",
    "\n",
    "# Validaciones\n",
    "if not RAW_MOD.exists():\n",
    "    raise FileNotFoundError(f\"No se encontró el modificado: {RAW_MOD}\")\n",
    "HAS_ORIG = RAW_ORI.exists()\n",
    "\n",
    "# Carga robusta\n",
    "df_mod = pd.read_csv(RAW_MOD, engine=\"python\")  # autodetecta separador\n",
    "print(\"df_mod:\", df_mod.shape)\n",
    "print(\"Primeras columnas:\", df_mod.columns[:10].tolist())\n",
    "\n",
    "# Si necesitas el original para validación:\n",
    "if HAS_ORIG:\n",
    "    df_orig = pd.read_csv(RAW_ORI, engine=\"python\")\n",
    "    print(\"df_orig:\", df_orig.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "executionInfo": {
     "elapsed": 797,
     "status": "ok",
     "timestamp": 1760206636547,
     "user": {
      "displayName": "Felipe de Jesús Gutiérrez Dávila",
      "userId": "01500989835494077702"
     },
     "user_tz": 360
    },
    "id": "p30_HO9mhWPP",
    "outputId": "bcb0fbd6-74f2-452a-aa72-2ca04bfe51ba",
    "papermill": {
     "duration": 0.012234,
     "end_time": "2025-10-12T21:17:12.657727",
     "exception": false,
     "start_time": "2025-10-12T21:17:12.645493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Carga de dataset\n",
    "# ============================================\n",
    "\n",
    "# Definir rutas de los datasets\n",
    "#path_modified = \"/content/drive/MyDrive/Colab Notebooks/Maestría MNA - IA Aplicada/TR5 - MLOPS/Fase 1/data/online_news_modified.csv\"\n",
    "#path_original = \"/content/drive/MyDrive/Colab Notebooks/Maestría MNA - IA Aplicada/TR5 - MLOPS/Fase 1/data/online_news_original.csv\"  # solo referencia, no se usará todavía\n",
    "\n",
    "#path_modified='/content/drive/MyDrive/Colab Notebooks/MNA/4rto_Trimestre/MLOps/Semana_4/Tarea/Steven Brutscher - A01732505/online_news_modified.csv'\n",
    "#path_original='/content/drive/MyDrive/Colab Notebooks/MNA/4rto_Trimestre/MLOps/Semana_4/Tarea/Steven Brutscher - A01732505/online_news_original.csv'\n",
    "\n",
    "# Cargar el dataset modificado (ruidoso)\n",
    "#df_mod = pd.read_csv(path_modified)\n",
    "\n",
    "# Vista rápida de los datos\n",
    "#print(\"Dataset modificado cargado correctamente.\\n\")\n",
    "#print(f\"Shape: {df_mod.shape[0]} rows × {df_mod.shape[1]} columns\\n\")\n",
    "#print(\"Columnas principales:\\n\", df_mod.columns[:10].tolist())\n",
    "\n",
    "# Mostrar primeras filas\n",
    "#df_mod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1911,
     "status": "ok",
     "timestamp": 1760206638458,
     "user": {
      "displayName": "Felipe de Jesús Gutiérrez Dávila",
      "userId": "01500989835494077702"
     },
     "user_tz": 360
    },
    "id": "45P6xaIohizb",
    "outputId": "498d5620-2ab0-4d2b-c24f-e2dcd656a58e",
    "papermill": {
     "duration": 1.459641,
     "end_time": "2025-10-12T21:17:14.123839",
     "exception": false,
     "start_time": "2025-10-12T21:17:12.664198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Verificación inicial del dataset modificado\n",
    "# ============================================\n",
    "\n",
    "print(\"============================================\")\n",
    "print(\"Basic Overview of the Modified Dataset\")\n",
    "print(\"============================================\\n\")\n",
    "\n",
    "# 1. Estructura general\n",
    "print(\"- General structure\")\n",
    "print(f\"Shape → {df_mod.shape[0]} rows × {df_mod.shape[1]} columns\\n\")\n",
    "\n",
    "# 2. Tipos de datos\n",
    "print(\"- Data Types:\")\n",
    "print(df_mod.dtypes.value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Mostrar las primeras filas de tipos no numéricos o mezclados\n",
    "print(\"- Columns with object (non-numeric) data types:\")\n",
    "print(df_mod.select_dtypes(include='object').columns.tolist())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3. Conteo de valores nulos\n",
    "print(\"- Missing Values (Top 10):\")\n",
    "missing = df_mod.isnull().sum().sort_values(ascending=False)\n",
    "print(missing.head(10))\n",
    "print(f\"\\n- Total columns with missing values: {(missing > 0).sum()} / {df_mod.shape[1]}\\n\")\n",
    "\n",
    "# 4. Conteo de filas duplicadas\n",
    "duplicated_rows = df_mod.duplicated().sum()\n",
    "print(f\"- Duplicate Rows → {duplicated_rows}\\n\")\n",
    "\n",
    "# 5. Estadísticas descriptivas básicas\n",
    "print(\"- Descriptive Statistics (Numeric Columns):\")\n",
    "display(df_mod.describe().T.head(10))  # se muestra solo un preview\n",
    "\n",
    "# 6. Identificación de columnas con tipo erróneo\n",
    "print(\"Potential issues: columns with mixed or inconsistent data types\")\n",
    "\n",
    "for col in df_mod.columns:\n",
    "    unique_types = df_mod[col].apply(lambda x: type(x).__name__).unique()\n",
    "    if len(unique_types) > 1:\n",
    "        print(f\" - Column '{col}' has mixed types: {unique_types}\")\n",
    "\n",
    "print(\"\\nInitial verification complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "1atB2yrUGDkM",
    "papermill": {
     "duration": 0.014878,
     "end_time": "2025-10-12T21:17:14.146691",
     "exception": false,
     "start_time": "2025-10-12T21:17:14.131813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Preliminary Findings from the Basic Overview**\n",
    "\n",
    "The initial verification of the modified dataset revealed a **severe degree of data corruption** intentionally introduced by the course instructors.  \n",
    "This step aimed to simulate a realistic data-cleaning scenario, where raw data obtained from external sources (e.g., APIs, user-generated content, or web scrapers) often contains inconsistencies, missing values, and format errors.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Observations**\n",
    "\n",
    "1. **All columns are stored as `object` type**, meaning that even numerical variables were read as text.  \n",
    "   This prevents standard numerical operations (such as correlations, distributions, or scaling).\n",
    "\n",
    "2. **Every single column (62 out of 62)** contains **missing or invalid values**, confirming that the dataset requires extensive cleaning.\n",
    "\n",
    "3. **Mixed data types** within single columns were detected, for example, strings and floats coexisting in the same field.  \n",
    "   Typical invalid entries include textual noise such as `\"error\"`, `\"NaN\"`, `\"unknown\"`, `\"bad\"`, or numeric values outside of their theoretical range (e.g., polarities < -1 or > 1).\n",
    "\n",
    "4. **No duplicated rows** were found, indicating that data corruption is structural (inside the cells), not redundant (between rows).\n",
    "\n",
    "5. A new, **spurious column named `mixed_type_col`** was identified. This column has no correspondence in the original dataset and contains mostly missing or meaningless entries.\n",
    "\n",
    "---\n",
    "\n",
    "### **Implications**\n",
    "\n",
    "These findings confirm that the modified dataset cannot yet be analyzed or modeled reliably.  \n",
    "Before any statistical insights or visualizations can be trusted, it is essential to:\n",
    "- Identify which variables should be numeric or categorical.  \n",
    "- Convert data types properly.  \n",
    "- Handle missing and inconsistent values.  \n",
    "- Detect outliers and invalid ranges.\n",
    "\n",
    "However, as part of good data analysis practice, it is helpful to first **visualize the dataset “as is”**, to understand the overall structure and to document the extent of corruption before applying any cleaning transformations.\n",
    "\n",
    "---\n",
    "\n",
    "The following section performs a **minimal global visualization** to inspect null distributions, column cardinalities, and general irregularities, providing a “snapshot” of the dataset’s initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4528,
     "status": "ok",
     "timestamp": 1760206642987,
     "user": {
      "displayName": "Felipe de Jesús Gutiérrez Dávila",
      "userId": "01500989835494077702"
     },
     "user_tz": 360
    },
    "id": "St5oMZgymGpN",
    "outputId": "0b84ce61-dc4b-404c-c042-c82abadb2ae4",
    "papermill": {
     "duration": 2.815374,
     "end_time": "2025-10-12T21:17:16.971426",
     "exception": false,
     "start_time": "2025-10-12T21:17:14.156052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Minimal Global Visualization (Pre-Cleaning)\n",
    "# ============================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Heatmap de valores nulos\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.heatmap(df_mod.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
    "plt.title('Missing Values Across Columns (Before Cleaning)', fontsize=14)\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# 2. Porcentaje de valores nulos por columna\n",
    "missing_percent = df_mod.isnull().mean() * 100\n",
    "plt.figure(figsize=(10, 5))\n",
    "missing_percent.sort_values(ascending=False).head(15).plot(kind='bar', color='firebrick')\n",
    "plt.title('Top 15 Columns with Highest Missing Percentage', fontsize=14)\n",
    "plt.ylabel('Percentage of Missing Values (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3. Conteo de cardinalidad (número de valores únicos por columna)\n",
    "unique_counts = df_mod.nunique().sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(unique_counts, bins=30, color='steelblue')\n",
    "plt.title('Distribution of Column Cardinality (Number of Unique Values)', fontsize=14)\n",
    "plt.xlabel('Number of Unique Values')\n",
    "plt.ylabel('Count of Columns')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4. Conteo de tipos de valores (solo ilustrativo)\n",
    "sample_values = df_mod.sample(5).T.head(20)\n",
    "display(sample_values)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Global visualization complete. The dataset will now be cleaned step by step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "Cc5aPhgQxETP",
    "papermill": {
     "duration": 0.010324,
     "end_time": "2025-10-12T21:17:16.991618",
     "exception": false,
     "start_time": "2025-10-12T21:17:16.981294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Methodological flow for cleaning\n",
    "\n",
    "The global visualization confirmed that every column was loaded as `object`, many cells contain textual noise or impossible values, and missingness is broadly uniform across columns (about 1.1–1.2%), with the exception of `mixed_type_col` at roughly 10 percent. Proceeding column by column would be slow, error prone, and hard to reproduce. Instead, we will use an MLOps-friendly flow that combines a systematic first pass with targeted follow ups:\n",
    "\n",
    "1) **Global type conversion**\n",
    "\n",
    "   Convert every column that should be numeric using `pd.to_numeric(errors='coerce')`, coercing invalid strings into `NaN`. This gives us a consistent numeric matrix on which we can compute statistics and validate ranges.\n",
    "\n",
    "2) **Post-conversion diagnostics**\n",
    "\n",
    "   Quantify how many values were coerced per column; identify columns that became fully numeric; measure the new missingness caused by type repair.\n",
    "\n",
    "3) **Grouped cleaning rather than column-by-column**\n",
    "\n",
    "   • Continuous metrics: shares, keyword statistics `kw_*`, counts.  \n",
    "   • Bounded probabilities and sentiment features: `LDA_*`, polarity and subjectivity scores; enforce valid ranges.  \n",
    "   • Binary flags: `data_channel_is_*`, `weekday_is_*`, `is_weekend`; normalize to {0, 1}.  \n",
    "   • Non-predictive or spurious fields: keep `url` as identifier; quarantine or drop `mixed_type_col` if it remains meaningless.\n",
    "\n",
    "4) **Imputation and record handling**\n",
    "\n",
    "   Select imputation strategies per group, or remove irreparable rows if they violate basic integrity constraints.\n",
    "\n",
    "5) **Validation and export**\n",
    "\n",
    "   Re-profile the cleaned dataset, then compare against the original reference only at the end to validate distributions and cleaning quality.\n",
    "\n",
    "The next cell performs Step 1, a global and safe numeric conversion, and reports a detailed summary of what was coerced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "id": "KOPnB3niIU6I",
    "papermill": {
     "duration": 0.00965,
     "end_time": "2025-10-12T21:17:17.010047",
     "exception": false,
     "start_time": "2025-10-12T21:17:17.000397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 1. Global Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 789
    },
    "executionInfo": {
     "elapsed": 2370,
     "status": "ok",
     "timestamp": 1760206645358,
     "user": {
      "displayName": "Felipe de Jesús Gutiérrez Dávila",
      "userId": "01500989835494077702"
     },
     "user_tz": 360
    },
    "id": "NveXc3d8IAuS",
    "outputId": "51f02895-a438-4bb7-de8b-4a800627729a",
    "papermill": {
     "duration": 1.253761,
     "end_time": "2025-10-12T21:17:18.272445",
     "exception": false,
     "start_time": "2025-10-12T21:17:17.018684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# STEP 1: Global numeric conversion with diagnostics\n",
    "# ==================================================\n",
    "\n",
    "# Let's work on a copy to preserve the raw modified dataset\n",
    "df_stage1 = df_mod.copy()\n",
    "\n",
    "# Columns that must remain non-numeric\n",
    "non_numeric_keep = {'url'}  # URL is an identifier; we keep it as string\n",
    "# All the other 61 columns are supposed to be numeric\n",
    "\n",
    "# Prepare a results collector\n",
    "records = []\n",
    "\n",
    "for col in df_stage1.columns:\n",
    "    before_dtype = df_stage1[col].dtype\n",
    "\n",
    "    if col in non_numeric_keep:\n",
    "        after_dtype = before_dtype\n",
    "        n_total = len(df_stage1)\n",
    "        n_coerced = 0\n",
    "        n_before_na = df_stage1[col].isna().sum()\n",
    "        n_after_na  = n_before_na\n",
    "        records.append({\n",
    "            \"column\": col,\n",
    "            \"before_dtype\": str(before_dtype),\n",
    "            \"after_dtype\": str(after_dtype),\n",
    "            \"coerced_to_NaN\": n_coerced,\n",
    "            \"coerced_pct\": 0.0,\n",
    "            \"na_before\": n_before_na,\n",
    "            \"na_after\": n_after_na\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Try numeric conversion with coercion\n",
    "    series_before = df_stage1[col]\n",
    "    n_total = len(series_before)\n",
    "    na_before = series_before.isna().sum()\n",
    "\n",
    "    converted = pd.to_numeric(series_before, errors=\"coerce\")\n",
    "\n",
    "    # Values that became NaN due to coercion: entries that were not NaN before but are NaN after\n",
    "    coerced_mask = converted.isna() & series_before.notna()\n",
    "    n_coerced = coerced_mask.sum()\n",
    "    coerced_pct = 100.0 * n_coerced / n_total\n",
    "\n",
    "    # Assign the converted series back\n",
    "    df_stage1[col] = converted\n",
    "    after_dtype = df_stage1[col].dtype\n",
    "    na_after = df_stage1[col].isna().sum()\n",
    "\n",
    "    records.append({\n",
    "        \"column\": col,\n",
    "        \"before_dtype\": str(before_dtype),\n",
    "        \"after_dtype\": str(after_dtype),\n",
    "        \"coerced_to_NaN\": int(n_coerced),\n",
    "        \"coerced_pct\": coerced_pct,\n",
    "        \"na_before\": int(na_before),\n",
    "        \"na_after\": int(na_after)\n",
    "    })\n",
    "\n",
    "summary_conv = pd.DataFrame.from_records(records).sort_values(\"coerced_pct\", ascending=False)\n",
    "\n",
    "# Report\n",
    "print(\"Global numeric conversion complete.\")\n",
    "print(f\"Rows × Columns after conversion: {df_stage1.shape[0]} × {df_stage1.shape[1]}\")\n",
    "print(\"\\nData types after conversion (value counts):\")\n",
    "print(df_stage1.dtypes.value_counts())\n",
    "\n",
    "print(\"\\nTop 10 columns by percent of values coerced to NaN:\")\n",
    "display(summary_conv.head(10)[[\"column\", \"coerced_to_NaN\", \"coerced_pct\", \"na_before\", \"na_after\"]])\n",
    "\n",
    "print(\"\\nColumns that remain non-numeric by design:\")\n",
    "print(sorted(list(non_numeric_keep)))\n",
    "\n",
    "# Keep both the converted dataframe and the conversion summary for the next steps\n",
    "df_conv_summary = summary_conv.copy()\n",
    "df_clean_stage1 = df_stage1  # alias for subsequent cleaning steps\n",
    "\n",
    "# Preview a few columns to verify the effect\n",
    "preview_cols = [\"shares\", \"LDA_00\", \"avg_positive_polarity\", \"weekday_is_monday\", \"data_channel_is_socmed\"]\n",
    "preview_cols = [c for c in preview_cols if c in df_clean_stage1.columns]\n",
    "print(\"\\nPreview of selected columns after conversion:\")\n",
    "display(df_clean_stage1[preview_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "id": "tl6Ef6zrlSN3",
    "papermill": {
     "duration": 0.010406,
     "end_time": "2025-10-12T21:17:18.292762",
     "exception": false,
     "start_time": "2025-10-12T21:17:18.282356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary of the Global Numeric Conversion Process\n",
    "\n",
    "The global numeric conversion successfully standardized the data types across the entire modified dataset. Out of the 62 total columns, 61 were successfully converted to `float64` while the column `url` was intentionally preserved as `object` since it serves only as a unique identifier and is non-predictive.\n",
    "\n",
    "### Key findings:\n",
    "1. **Uniform data structure achieved.**  \n",
    "   Every numeric field is now ready for statistical profiling, imputation, and visualization. This marks a major milestone in transforming the dataset from a corrupted or noisy state into a usable analytical format.\n",
    "\n",
    "2. **Minimal coercion in most columns.**  \n",
    "   Nearly all features showed less than **0.5 %** of coerced values (i.e., entries that were invalid and replaced with `NaN` during conversion), confirming that the majority of injected noise was relatively mild.\n",
    "\n",
    "3. **One severely affected feature.**  \n",
    "   The column `mixed_type_col` presented approximately **19.7 %** invalid values, suggesting deliberate corruption. This variable will require special treatment (potentially exclusion) depending on its correlation and predictive power.\n",
    "\n",
    "4. **Integrity maintained.**  \n",
    "   The overall dataset shape (40,436 × 62) and column names remain identical to the original schema, which ensures downstream compatibility with modeling pipelines and external references.\n",
    "\n",
    "The next step is to assess how these conversions affected data completeness by quantifying and visualizing the resulting missing values across all columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "9trpIUjNIcL6",
    "papermill": {
     "duration": 0.00902,
     "end_time": "2025-10-12T21:17:18.310815",
     "exception": false,
     "start_time": "2025-10-12T21:17:18.301795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 2. Post conversion diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3486,
     "status": "ok",
     "timestamp": 1760206648845,
     "user": {
      "displayName": "Felipe de Jesús Gutiérrez Dávila",
      "userId": "01500989835494077702"
     },
     "user_tz": 360
    },
    "id": "2rUmstwn02E_",
    "outputId": "e3c2589b-ce11-4d30-bac9-6de99c454d63",
    "papermill": {
     "duration": 2.418547,
     "end_time": "2025-10-12T21:17:20.739732",
     "exception": false,
     "start_time": "2025-10-12T21:17:18.321185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# STEP 2: Post-Conversion Diagnostics & Visualization\n",
    "# ===================================================\n",
    "\n",
    "# Reuse df_clean_stage1 and df_conv_summary\n",
    "df_clean_stage1 = df_stage1.copy()\n",
    "summary_conv = summary_conv.copy()\n",
    "\n",
    "# ----- General Missing Values Summary -----\n",
    "total_missing = df_clean_stage1.isna().sum().sum()\n",
    "missing_by_col = df_clean_stage1.isna().sum().sort_values(ascending=False)\n",
    "missing_pct_by_col = (missing_by_col / len(df_clean_stage1)) * 100\n",
    "\n",
    "print(f\"\\nTotal missing values across entire dataset: {total_missing:,}\")\n",
    "print(f\"\\nAverage missing percentage per column: {missing_pct_by_col.mean():.2f}%\")\n",
    "print(f\"\\nColumns with >5% missing values: {(missing_pct_by_col > 5).sum()} / {len(df_clean_stage1.columns)}\")\n",
    "print(\"\\nTop 10 columns with highest missing percentages:\")\n",
    "display(missing_pct_by_col.head(10))\n",
    "\n",
    "# ----- Visualization 1: Histogram of Missing Percentages -----\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(missing_pct_by_col, bins=30, kde=False)\n",
    "plt.title(\"Distribution of Missing Values per Column (%)\")\n",
    "plt.xlabel(\"Percentage of Missing Values\")\n",
    "plt.ylabel(\"Number of Columns\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "# ----- Visualization 2: Horizontal Bar Plot (Top 15 Columns) -----\n",
    "plt.figure(figsize=(10,7))\n",
    "missing_pct_by_col.head(15).sort_values().plot(kind='barh', color='steelblue')\n",
    "plt.title(\"Top 15 Columns with Highest Missing Percentages\")\n",
    "plt.xlabel(\"Percentage of Missing Values\")\n",
    "plt.ylabel(\"Column\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "# ----- Visualization 3: Correlation Heatmap (NaN occurrence pattern) -----\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df_clean_stage1.isna(), cbar=False)\n",
    "plt.title(\"Heatmap of Missing Values across All Columns\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Samples\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPost-conversion diagnostic complete. The next step will be grouping variables by type and applying targeted cleaning or imputation strategies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "4qJuIcWOrBOE",
    "papermill": {
     "duration": 0.011947,
     "end_time": "2025-10-12T21:17:20.763173",
     "exception": false,
     "start_time": "2025-10-12T21:17:20.751226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Summary of the Post-Conversion Diagnostics\n",
    "After converting all columns (except `url`) to numeric format, the dataset presents the following characteristics:\n",
    "- **Total missing values:** 48,597  \n",
    "- **Average missing rate:** 1.94 %  \n",
    "- **Columns with more than 5 % missing data:** only one, `mixed_type_col` (≈ 29.8 %)  \n",
    "- All other features remain between **1.5 % – 1.6 %** missing, confirming a largely uniform corruption level across the dataset.\n",
    "\n",
    "The noise injection therefore affected all columns slightly and one column heavily, without altering the global structure (40,436 rows × 62 columns).  \n",
    "Visual inspection confirmed that the missing values are randomly distributed rather than concentrated by group or sample, which suggests stochastic noise instead of systemic bias.\n",
    "\n",
    "### Decision Regarding `mixed_type_col`\n",
    "Although `mixed_type_col` exhibits a substantially higher error rate, it will **not be removed immediately**.  \n",
    "In a real-world MLOps scenario, data scientists cannot assume prior knowledge of which features are synthetic or unreliable.  \n",
    "Therefore, the column will be retained temporarily and treated as a candidate variable to evaluate its relevance empirically through the following steps:\n",
    "\n",
    "1. **Correlation analysis** with the target variable `shares` and with other predictive features.  \n",
    "   - If its absolute correlation < 0.05, it will be flagged as statistically irrelevant.  \n",
    "2. **Model comparison experiments.**  \n",
    "   - Train two models: one including `mixed_type_col` and another excluding it.  \n",
    "   - Evaluate differences in performance metrics (e.g., R², MAE, RMSE).  \n",
    "3. **Documentation and reproducibility.**  \n",
    "   - All results and decisions will be version-controlled and justified according to MLOps principles of traceability and transparency.\n",
    "\n",
    "This approach aligns with professional standards in Machine Learning Operations: no feature is discarded without quantitative evidence of its insignificance or negative impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "e48J7e9WGZyj",
    "papermill": {
     "duration": 0.010709,
     "end_time": "2025-10-12T21:17:20.784533",
     "exception": false,
     "start_time": "2025-10-12T21:17:20.773824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 3. Grouped Cleaning: rationale and plan\n",
    "\n",
    "Following the global type conversion and the post-conversion diagnostics, we proceed with a grouped cleaning strategy. The goal is to enforce logical bounds, fix or mark invalid values consistently, and prepare the data for imputation. Cleaning by groups is preferred over column-by-column because many features share the same constraints and semantics; this reduces manual effort, increases consistency, and improves reproducibility.\n",
    "\n",
    "Groups and rules (refined with the Fernandes–Vinagre–Cortez paper):\n",
    "\n",
    "1) Continuous metrics  \n",
    "   Examples: `shares`, `kw_*`, `self_reference_*`, `n_tokens_*`, `average_token_length`, `num_*`, `num_keywords`, `timedelta`.  \n",
    "   Baseline rules: no negatives for counts; detect outliers using IQR or percentile thresholds; for `timedelta`, valid range is [8, 731] (values outside are invalid).  \n",
    "   Special case for keywords: in `kw_min_min`, `kw_avg_min`, `kw_min_avg`, negative values are sentinel codes for “unknown/missing”; we will treat them as missing rather than as sign errors. All other `kw_*` and every `self_reference_*` must be non-negative.\n",
    "\n",
    "2) Bounded probabilities and sentiment features  \n",
    "   LDA topics: `LDA_00`–`LDA_04`, each in [0, 1], and the per-row sum should be ≈ 1.0 (tolerance ±0.05).  \n",
    "   Subjectivity and rates: `global_subjectivity`, `title_subjectivity`, `global_rate_positive_words`, `global_rate_negative_words`, `rate_positive_words`, `rate_negative_words` ∈ [0, 1].  \n",
    "   Polarities:  \n",
    "   • Positive polarities (`avg_positive_polarity`, `min_positive_polarity`, `max_positive_polarity`) ∈ [0, 1].  \n",
    "   • Negative polarities (`avg_negative_polarity`, `min_negative_polarity`, `max_negative_polarity`) ∈ [-1, 0].  \n",
    "   • Global/title polarities (`global_sentiment_polarity`, `title_sentiment_polarity`) ∈ [-1, 1].  \n",
    "   Consistency: `abs_title_sentiment_polarity` should equal `abs(title_sentiment_polarity)`; we will flag mismatches.\n",
    "\n",
    "3) Binary flags  \n",
    "   Channels: `data_channel_is_*`; Weekdays: `weekday_is_*`; Weekend flag: `is_weekend`.  \n",
    "   Rules: values must be in {0, 1}. Exclusivity constraints: exactly one weekday equals 1 per row; at most one channel equals 1 per row; if Saturday or Sunday is 1, then `is_weekend` must be 1.\n",
    "\n",
    "4) Non-predictive or spurious fields  \n",
    "   Keep `url` as an identifier. Keep `timedelta` with range enforcement. Keep `mixed_type_col` provisionally; we will evaluate its empirical relevance later. If it proves useless or harmful, we will drop it with justification.\n",
    "\n",
    "Duplicates policy:  \n",
    "If multiple rows share the same `url` but differ in numeric values, we keep the row with the highest `shares`. This choice is easy to justify for a popularity target, it resolves contradictions deterministically, and it keeps one canonical record per article.\n",
    "\n",
    "This step will produce diagnostics per group, count violations according to the refined rules, and prepare a cleaned dataframe for the subsequent steps: imputation and record handling (Step 4), then validation and export (Step 5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1760206649056,
     "user": {
      "displayName": "Felipe de Jesús Gutiérrez Dávila",
      "userId": "01500989835494077702"
     },
     "user_tz": 360
    },
    "id": "6zuJ9P_lnGjq",
    "outputId": "e005165c-4875-4af2-ac05-08d6b6a6e158",
    "papermill": {
     "duration": 0.179236,
     "end_time": "2025-10-12T21:17:20.982918",
     "exception": false,
     "start_time": "2025-10-12T21:17:20.803682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 3.1 — Grouped Cleaning: deduplication & refined diagnostics\n",
    "# ============================================================\n",
    "\n",
    "# 0) Cargamos el dataframe de trabajo a partir del Stage 1 (numérico) si no existe\n",
    "if \"df_clean_stage1\" not in globals():\n",
    "    df_clean_stage1 = df_mod.copy()\n",
    "    for c in df_clean_stage1.columns:\n",
    "        if c != \"url\":\n",
    "            df_clean_stage1[c] = pd.to_numeric(df_clean_stage1[c], errors=\"coerce\")\n",
    "\n",
    "df_work = df_clean_stage1.copy()\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Deduplicate by URL policy\n",
    "# -----------------------------\n",
    "df_work['url']=df_work['url'].astype(str)\n",
    "df_work['url']=[url.strip().lower() if url.strip().lower().startswith('http:') else np.nan for url in df_work['url']]\n",
    "if \"url\" in df_work.columns:\n",
    "    if \"shares\" in df_work.columns:\n",
    "        df_work = (\n",
    "            df_work.sort_values([\"url\", \"shares\"], ascending=[True, True])\n",
    "                   .drop_duplicates(subset=\"url\", keep=\"last\")\n",
    "                   .reset_index(drop=True)\n",
    "        )\n",
    "    else:\n",
    "        df_work = df_work.drop_duplicates(subset=\"url\", keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "df_work=df_work.dropna(subset='url')\n",
    "df_work['url']=df_work['url'].astype(str)\n",
    "\n",
    "print(f\"After URL deduplication: {df_work.shape[0]} rows × {df_work.shape[1]} columns\")\n",
    "\n",
    "# ------------------------------------\n",
    "# 2) Define column groups dynamically\n",
    "# ------------------------------------\n",
    "cols = df_work.columns.tolist()\n",
    "\n",
    "lda_cols      = [c for c in cols if c.startswith(\"LDA_\")]\n",
    "chan_cols     = [c for c in cols if c.startswith(\"data_channel_is_\")]\n",
    "wday_cols     = [c for c in cols if c.startswith(\"weekday_is_\")]\n",
    "rate_cols     = [c for c in cols if c in [\n",
    "    \"global_rate_positive_words\", \"global_rate_negative_words\",\n",
    "    \"rate_positive_words\", \"rate_negative_words\"\n",
    "]]\n",
    "subj_cols     = [c for c in cols if c in [\n",
    "    \"global_subjectivity\", \"title_subjectivity\", \"abs_title_subjectivity\"\n",
    "]]\n",
    "pol_pos_cols  = [c for c in cols if c in [\n",
    "    \"avg_positive_polarity\", \"min_positive_polarity\", \"max_positive_polarity\"\n",
    "]]\n",
    "pol_neg_cols  = [c for c in cols if c in [\n",
    "    \"avg_negative_polarity\", \"min_negative_polarity\", \"max_negative_polarity\"\n",
    "]]\n",
    "pol_any_cols  = [c for c in cols if c in [\n",
    "    \"global_sentiment_polarity\", \"title_sentiment_polarity\"\n",
    "]]\n",
    "kw_cols       = [c for c in cols if c.startswith(\"kw_\")]\n",
    "kw_neg_as_na  = {\"kw_min_min\", \"kw_avg_min\", \"kw_min_avg\"}  # negativos considerados \"missing\"\n",
    "selfref_cols  = [c for c in cols if c.startswith(\"self_reference_\")]\n",
    "count_cols    = [c for c in cols if c.startswith(\"n_tokens_\")] + \\\n",
    "                [c for c in cols if c.startswith(\"num_\")] + [\"num_keywords\", \"average_token_length\"]\n",
    "bin_cols      = list(sorted(set(chan_cols + wday_cols + [\"is_weekend\"]).intersection(cols)))\n",
    "\n",
    "# continuous metrics, excluding already listed groups and obvious non-numeric\n",
    "exclude = set([\"url\"] + lda_cols + chan_cols + wday_cols + rate_cols + subj_cols +\n",
    "              pol_pos_cols + pol_neg_cols + pol_any_cols + kw_cols + selfref_cols +\n",
    "              count_cols + bin_cols)\n",
    "cont_cols = [c for c in cols if c not in exclude]\n",
    "\n",
    "has_timedelta = \"timedelta\" in df_work.columns\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3) Diagnostics: bounds and logical constraints per group\n",
    "# ----------------------------------------------------------\n",
    "violations = {}\n",
    "\n",
    "# a) Continuous-like and counts: negatives invalid (except kw_neg_as_na handled below)\n",
    "def count_negatives(df, columns):\n",
    "    bad = {}\n",
    "    for c in columns:\n",
    "        if c in df:\n",
    "            s = df[c]\n",
    "            bad[c] = int(((s < 0) & s.notna()).sum())\n",
    "    return {k: v for k, v in bad.items() if v > 0}\n",
    "\n",
    "# For counts and self_reference_* and general continuous; do NOT include kw_* here\n",
    "cont_neg = {}\n",
    "cont_neg.update(count_negatives(df_work, cont_cols))\n",
    "cont_neg.update(count_negatives(df_work, selfref_cols))\n",
    "cont_neg.update(count_negatives(df_work, count_cols))\n",
    "# kw_* negatives are handled with special rule; exclude kw_min_min, kw_avg_min, kw_min_avg from \"error\" tallies\n",
    "kw_neg_regular = [c for c in kw_cols if c not in kw_neg_as_na]\n",
    "cont_neg.update(count_negatives(df_work, kw_neg_regular))\n",
    "violations[\"continuous_negatives\"] = cont_neg\n",
    "\n",
    "# Special rule for timedelta\n",
    "if has_timedelta:\n",
    "    td = df_work[\"timedelta\"]\n",
    "    bad_td = int((((td < 8) | (td > 731)) & td.notna()).sum())\n",
    "    violations[\"timedelta_out_of_range\"] = {\"timedelta\": bad_td}\n",
    "\n",
    "# Record how many negatives exist in the \"sentinel-as-missing\" kw columns (for transparency)\n",
    "kw_neg_missing_counts = {}\n",
    "for c in kw_neg_as_na:\n",
    "    if c in df_work.columns:\n",
    "        s = df_work[c]\n",
    "        kw_neg_missing_counts[c] = int(((s < 0) & s.notna()).sum())\n",
    "violations[\"kw_negatives_treated_as_missing\"] = {k: v for k, v in kw_neg_missing_counts.items() if v > 0}\n",
    "\n",
    "# b) Bounded in [0, 1]: LDA and rates/subjectivity\n",
    "def count_out_of_01(df, columns):\n",
    "    bad = {}\n",
    "    for c in columns:\n",
    "        if c in df:\n",
    "            s = df[c]\n",
    "            bad[c] = int((((s < 0) | (s > 1)) & s.notna()).sum())\n",
    "    return {k: v for k, v in bad.items() if v > 0}\n",
    "\n",
    "violations[\"lda_out_of_[0,1]\"]   = count_out_of_01(df_work, lda_cols)\n",
    "violations[\"rates_out_of_[0,1]\"] = count_out_of_01(df_work, rate_cols + subj_cols)\n",
    "\n",
    "# c) Polarities: positive in [0,1], negative in [-1,0], global/title in [-1,1]\n",
    "def count_out_of_range(df, columns, lo, hi):\n",
    "    bad = {}\n",
    "    for c in columns:\n",
    "        if c in df:\n",
    "            s = df[c]\n",
    "            bad[c] = int((((s < lo) | (s > hi)) & s.notna()).sum())\n",
    "    return {k: v for k, v in bad.items() if v > 0}\n",
    "\n",
    "violations[\"positive_polarities_out_of_[0,1]\"] = count_out_of_range(df_work, pol_pos_cols, 0.0, 1.0)\n",
    "violations[\"negative_polarities_out_of_[-1,0]\"] = count_out_of_range(df_work, pol_neg_cols, -1.0, 0.0)\n",
    "violations[\"global_title_polarities_out_of_[-1,1]\"] = count_out_of_range(df_work, pol_any_cols, -1.0, 1.0)\n",
    "\n",
    "# d) LDA sum per row ≈ 1.0\n",
    "if len(lda_cols) == 5 and all(c in df_work for c in lda_cols):\n",
    "    lda_sum = df_work[lda_cols].sum(axis=1)\n",
    "    lda_bad_sum = int((((lda_sum < 0.95) | (lda_sum > 1.05)) & lda_sum.notna()).sum())\n",
    "    violations[\"lda_sum_not_close_to_1\"] = {\"rows_violating\": lda_bad_sum}\n",
    "\n",
    "# e) abs(title_sentiment_polarity) consistency\n",
    "if {\"abs_title_sentiment_polarity\", \"title_sentiment_polarity\"}.issubset(df_work.columns):\n",
    "    abs_ts = df_work[\"abs_title_sentiment_polarity\"]\n",
    "    ts    = df_work[\"title_sentiment_polarity\"]\n",
    "    # count mismatches where both are present\n",
    "    mask = abs_ts.notna() & ts.notna()\n",
    "    mism = int((~np.isclose(abs_ts[mask], ts[mask].abs())).sum())\n",
    "    violations[\"abs_title_sentiment_mismatch\"] = {\"rows_violating\": mism}\n",
    "\n",
    "# f) Binary flags: non {0, 1}\n",
    "bin_bad = {}\n",
    "for c in bin_cols:\n",
    "    s = df_work[c]\n",
    "    bad_count = int(((~s.isin([0.0, 1.0])) & s.notna()).sum())\n",
    "    if bad_count > 0:\n",
    "        bin_bad[c] = bad_count\n",
    "violations[\"binary_not_in_{0,1}\"] = bin_bad\n",
    "\n",
    "# f.1) Exclusivity: weekdays, exactly one equals 1\n",
    "if len(wday_cols) >= 5:\n",
    "    wday_sum = df_work[wday_cols].sum(axis=1)\n",
    "    wday_bad = int(((wday_sum != 1.0) & wday_sum.notna()).sum())\n",
    "    violations[\"weekday_exclusivity\"] = {\"rows_violating\": wday_bad}\n",
    "\n",
    "# f.2) Exclusivity: channels, at most one equals 1\n",
    "if len(chan_cols) >= 2:\n",
    "    chan_sum = df_work[chan_cols].sum(axis=1)\n",
    "    chan_bad = int(((chan_sum > 1.0) & chan_sum.notna()).sum())\n",
    "    violations[\"channel_at_most_one\"] = {\"rows_violating\": chan_bad}\n",
    "\n",
    "# f.3) Weekend consistency\n",
    "if {\"is_weekend\", \"weekday_is_saturday\", \"weekday_is_sunday\"}.issubset(df_work.columns):\n",
    "    weekend_expected = ((df_work[\"weekday_is_saturday\"] == 1.0) | (df_work[\"weekday_is_sunday\"] == 1.0)).astype(float)\n",
    "    mask = weekend_expected.notna() & df_work[\"is_weekend\"].notna()\n",
    "    weekend_inconsistent = int((df_work.loc[mask, \"is_weekend\"] != weekend_expected[mask]).sum())\n",
    "    violations[\"weekend_consistency\"] = {\"rows_violating\": weekend_inconsistent}\n",
    "\n",
    "# -----------------------\n",
    "# 4) Summaries to review\n",
    "# -----------------------\n",
    "def nonempty_dict(d):\n",
    "    return {k: v for k, v in d.items() if (isinstance(v, dict) and len(v) > 0) or (isinstance(v, int) and v > 0)}\n",
    "\n",
    "print(\"\\nGrouped cleaning diagnostics, counts of violations by rule:\")\n",
    "for k, v in violations.items():\n",
    "    if isinstance(v, dict):\n",
    "        v_filtered = nonempty_dict(v)\n",
    "        if len(v_filtered) > 0:\n",
    "            print(f\"- {k}:\")\n",
    "            items = sorted(v_filtered.items(), key=lambda x: x[1], reverse=True)\n",
    "            for name, count in items[:15]:\n",
    "                print(f\"    {name}: {count}\")\n",
    "    elif isinstance(v, int):\n",
    "        if v > 0:\n",
    "            print(f\"- {k}: {v}\")\n",
    "    else:\n",
    "        print(f\"- {k}: {v}\")\n",
    "\n",
    "# Keep the working dataframe for the next substeps in Step 3\n",
    "df_clean_stage2 = df_work.copy()\n",
    "\n",
    "print(\"\\nPrepared df_clean_stage2 for subsequent cleaning actions, \"\n",
    "      \"such as enforcing bounds, repairing binaries, and marking irreparable rows for removal or imputation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "iI0zBfwQhXg_",
    "papermill": {
     "duration": 0.012257,
     "end_time": "2025-10-12T21:17:21.006590",
     "exception": false,
     "start_time": "2025-10-12T21:17:20.994333",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Grouped Cleaning: results and transition to enforcement\n",
    "\n",
    "After running the grouped diagnostics, we identified the following integrity issues that guide the next substep, enforcement. The goal is to repair, normalize, or flag values consistently across feature groups so the dataset is ready for imputation.\n",
    "\n",
    "### Summary of key findings\n",
    "\n",
    "1) Deduplication\n",
    "- The dataset was reduced to **39,314 rows** by keeping, for each `url`, the record with the highest `shares`. This yields one canonical row per article.\n",
    "\n",
    "2) Continuous features and counts\n",
    "- `timedelta`: **312** rows outside the valid range [8, 731]; these will become `NaN`.\n",
    "- Keyword sentinels: negatives in `kw_min_min` (**22,342**), `kw_avg_min` (**807**), and `kw_min_avg` (**5**) represent unknown or missing values. These will be treated as `NaN`, not as sign errors.\n",
    "- Other `kw_*`, every `self_reference_*`, and count-like features must be non-negative. Any negatives will be set to `NaN`.\n",
    "\n",
    "3) LDA topic distributions\n",
    "- Values outside [0, 1] appear across all `LDA_*` columns.\n",
    "- **3,159** rows have the per-row LDA sum far from 1.0.\n",
    "- Plan: out-of-range entries will be nulled, then rows with positive sums will be renormalized so `LDA_00+...+LDA_04 = 1`.\n",
    "\n",
    "4) Rates, subjectivity, and sentiment bounds\n",
    "- Several hundred rows violate the [0, 1] bounds for `global_rate_positive_words`, `global_rate_negative_words`, `rate_positive_words`, `rate_negative_words`, `global_subjectivity`, `title_subjectivity`, `abs_title_subjectivity`. These will be set to `NaN`.\n",
    "- Polarity constraints:\n",
    "  - Positive polarities (`avg_positive_polarity`, `min_positive_polarity`, `max_positive_polarity`) must lie in [0, 1].\n",
    "  - Negative polarities (`avg_negative_polarity`, `min_negative_polarity`, `max_negative_polarity`) must lie in [−1, 0].\n",
    "  - Global and title polarities (`global_sentiment_polarity`, `title_sentiment_polarity`) must lie in [−1, 1].\n",
    "  - Out-of-range values will be set to `NaN`.\n",
    "\n",
    "5) Derived absolute sentiment\n",
    "- `abs_title_sentiment_polarity` mismatches `abs(title_sentiment_polarity)` in **540** rows. It will be corrected where both exist and filled from `abs(title_sentiment_polarity)` when only the latter exists.\n",
    "\n",
    "6) Binary flags and logical consistency\n",
    "- Many binary flags are not in {0, 1}.\n",
    "- Weekday exclusivity violations: **2,115** rows.\n",
    "- Channel at-most-one violations: **1,302** rows.\n",
    "- Weekend consistency violations: **362** rows.\n",
    "- Plan: snap near-binary values to {0, 1} with a tolerance, repair exclusivity by keeping the argmax, and enforce `is_weekend = 1` whenever Saturday or Sunday equals 1.\n",
    "\n",
    "### Next step: enforcement\n",
    "\n",
    "The next cell will apply these rules automatically and produce `df_clean_stage3`. It will:\n",
    "- Convert sentinel negatives and out-of-range values to `NaN`.\n",
    "- Renormalize valid LDA rows to sum to 1.\n",
    "- Enforce polarity and probability bounds.\n",
    "- Snap binary flags to {0, 1} and repair weekday and channel exclusivity, as well as weekend consistency.\n",
    "\n",
    "This prepares the dataset for Step 4, imputation and record handling.\n",
    "\n",
    "### NOTE: LDA topic normalization rationale\n",
    "\n",
    "Each of the five LDA topic features (`LDA_00`–`LDA_04`) represents a probability weight indicating the degree of association of an article with a latent topic.  \n",
    "By definition, each value must lie within **[0, 1]**, and the **sum of all five must be approximately 1.0**.\n",
    "\n",
    "During the following enforcement, any values outside [0, 1] will be replaced with `NaN`, and rows whose LDA topics summed to less than 0.95 or greater than 1.05 will be **renormalized** so that their valid values summed to ≈1.  \n",
    "Rows that were already within tolerance will be left unchanged, ensuring we preserve valid topic proportions.  \n",
    "This normalization guarantees internal consistency across the LDA features while retaining the statistical meaning of each topic distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1760206649314,
     "user": {
      "displayName": "Felipe de Jesús Gutiérrez Dávila",
      "userId": "01500989835494077702"
     },
     "user_tz": 360
    },
    "id": "rNAucwRDJg-f",
    "outputId": "0df0bb70-02da-427c-c75c-085619f92cea",
    "papermill": {
     "duration": 0.171669,
     "end_time": "2025-10-12T21:17:21.189013",
     "exception": false,
     "start_time": "2025-10-12T21:17:21.017344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 3.2 — Enforcement: apply group rules and produce df_clean_stage3\n",
    "# ============================================================\n",
    "\n",
    "df_enf = df_clean_stage2.copy()\n",
    "fix_report = []\n",
    "\n",
    "def add_fix(msg, n):\n",
    "    if n and n > 0:\n",
    "        fix_report.append((msg, int(n)))\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Timedelta in [8, 731]\n",
    "# ---------------------------\n",
    "if \"timedelta\" in df_enf.columns:\n",
    "    s = df_enf[\"timedelta\"]\n",
    "    mask = s.notna() & ((s < 8) | (s > 731))\n",
    "    n = mask.sum()\n",
    "    df_enf.loc[mask, \"timedelta\"] = np.nan\n",
    "    add_fix(\"timedelta out of [8,731] -> NaN\", n)\n",
    "\n",
    "# -----------------------------------\n",
    "# 2) Keyword stats and self_reference\n",
    "# -----------------------------------\n",
    "kw_cols      = [c for c in df_enf.columns if c.startswith(\"kw_\")]\n",
    "kw_neg_na    = {\"kw_min_min\", \"kw_avg_min\", \"kw_min_avg\"}  # negativos = missing\n",
    "selfref_cols = [c for c in df_enf.columns if c.startswith(\"self_reference_\")]\n",
    "\n",
    "for c in kw_cols:\n",
    "    if c not in df_enf:\n",
    "        continue\n",
    "    s = df_enf[c]\n",
    "    mask_neg = s.notna() & (s < 0)\n",
    "    n = mask_neg.sum()\n",
    "    # en las tres columnas especiales, negativo = missing; en el resto, negativo inválido también -> NaN\n",
    "    df_enf.loc[mask_neg, c] = np.nan\n",
    "    add_fix(f\"{c}: negatives -> NaN (sentinel or invalid)\", n)\n",
    "\n",
    "for c in selfref_cols:\n",
    "    s = df_enf[c]\n",
    "    mask_neg = s.notna() & (s < 0)\n",
    "    n = mask_neg.sum()\n",
    "    df_enf.loc[mask_neg, c] = np.nan\n",
    "    add_fix(f\"{c}: negatives invalid -> NaN\", n)\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Probabilities and rates\n",
    "# ---------------------------\n",
    "lda_cols  = [c for c in df_enf.columns if c.startswith(\"LDA_\")]\n",
    "rate_cols = [\"global_rate_positive_words\", \"global_rate_negative_words\",\n",
    "             \"rate_positive_words\", \"rate_negative_words\"]\n",
    "subj_cols = [\"global_subjectivity\", \"title_subjectivity\", \"abs_title_subjectivity\"]\n",
    "\n",
    "def clamp_01(columns, label):\n",
    "    total = 0\n",
    "    for c in columns:\n",
    "        if c in df_enf.columns:\n",
    "            s = df_enf[c]\n",
    "            mask = s.notna() & ((s < 0) | (s > 1))\n",
    "            n = mask.sum()\n",
    "            if n:\n",
    "                df_enf.loc[mask, c] = np.nan\n",
    "                total += n\n",
    "    add_fix(f\"{label}: out of [0,1] -> NaN\", total)\n",
    "\n",
    "clamp_01(lda_cols, \"LDA\")\n",
    "clamp_01(rate_cols + subj_cols, \"Rates/Subjectivities\")\n",
    "\n",
    "# LDA renormalization: filas con suma > 0 se normalizan a 1\n",
    "if len(lda_cols) == 5 and all(c in df_enf.columns for c in lda_cols):\n",
    "    lda_mat = df_enf[lda_cols]\n",
    "    lda_sum = lda_mat.sum(axis=1)\n",
    "    mask_ok = lda_sum.notna() & (lda_sum > 0)\n",
    "    df_enf.loc[mask_ok, lda_cols] = lda_mat[mask_ok].div(lda_sum[mask_ok], axis=0)\n",
    "    add_fix(\"LDA rows renormalized to sum=1\", int(mask_ok.sum()))\n",
    "\n",
    "# ---------------------------------\n",
    "# 4) Polarity constraints\n",
    "# ---------------------------------\n",
    "pol_pos = [\"avg_positive_polarity\", \"min_positive_polarity\", \"max_positive_polarity\"]\n",
    "pol_neg = [\"avg_negative_polarity\", \"min_negative_polarity\", \"max_negative_polarity\"]\n",
    "pol_any = [\"global_sentiment_polarity\", \"title_sentiment_polarity\"]\n",
    "\n",
    "def clamp_range(cols, lo, hi, label):\n",
    "    total = 0\n",
    "    for c in cols:\n",
    "        if c in df_enf.columns:\n",
    "            s = df_enf[c]\n",
    "            mask = s.notna() & ((s < lo) | (s > hi))\n",
    "            n = mask.sum()\n",
    "            if n:\n",
    "                df_enf.loc[mask, c] = np.nan\n",
    "                total += n\n",
    "    add_fix(f\"{label}: out of [{lo},{hi}] -> NaN\", total)\n",
    "\n",
    "clamp_range(pol_pos, 0.0, 1.0, \"Positive polarities\")\n",
    "clamp_range(pol_neg, -1.0, 0.0, \"Negative polarities\")\n",
    "clamp_range(pol_any, -1.0, 1.0, \"Global/Title polarities\")\n",
    "\n",
    "# abs(title_sentiment_polarity) consistency\n",
    "if {\"abs_title_sentiment_polarity\", \"title_sentiment_polarity\"}.issubset(df_enf.columns):\n",
    "    ts    = df_enf[\"title_sentiment_polarity\"]\n",
    "    abs_t = df_enf[\"abs_title_sentiment_polarity\"]\n",
    "    exp   = ts.abs()\n",
    "    # corregir donde ambos existen y difieren\n",
    "    mask_fix = ts.notna() & abs_t.notna() & (~np.isclose(abs_t, exp))\n",
    "    n = mask_fix.sum()\n",
    "    df_enf.loc[mask_fix, \"abs_title_sentiment_polarity\"] = exp[mask_fix]\n",
    "    add_fix(\"abs_title_sentiment_polarity corrected to abs(title_sentiment_polarity)\", n)\n",
    "    # rellenar donde abs está NaN y ts existe\n",
    "    mask_fill = ts.notna() & df_enf[\"abs_title_sentiment_polarity\"].isna()\n",
    "    n2 = mask_fill.sum()\n",
    "    df_enf.loc[mask_fill, \"abs_title_sentiment_polarity\"] = exp[mask_fill]\n",
    "    add_fix(\"abs_title_sentiment_polarity filled from abs(title_sentiment_polarity)\", n2)\n",
    "\n",
    "# ---------------------------------\n",
    "# 5) Binary flags and exclusivities\n",
    "# ---------------------------------\n",
    "bin_cols = [c for c in df_enf.columns if c.startswith(\"data_channel_is_\")] + \\\n",
    "           [c for c in df_enf.columns if c.startswith(\"weekday_is_\")] + [\"is_weekend\"]\n",
    "bin_cols = [c for c in bin_cols if c in df_enf.columns]\n",
    "\n",
    "# Snap a {0,1} con tolerancia; lo demás -> NaN\n",
    "for c in bin_cols:\n",
    "    s = df_enf[c]\n",
    "    snapped = s.copy()\n",
    "    snapped[(s >= 0.9) & (s <= 1.1)] = 1.0\n",
    "    snapped[(s >= -0.1) & (s <= 0.1)] = 0.0\n",
    "    mask_bad = snapped.notna() & (~snapped.isin([0.0, 1.0]))\n",
    "    n = mask_bad.sum()\n",
    "    snapped[mask_bad] = np.nan\n",
    "    df_enf[c] = snapped\n",
    "    add_fix(f\"{c}: snapped to {{0,1}} or set to NaN\", n)\n",
    "\n",
    "# Weekday exclusivity: si hay múltiples 1, quedarnos con el argmax\n",
    "wday_cols = [c for c in df_enf.columns if c.startswith(\"weekday_is_\")]\n",
    "if len(wday_cols) >= 5:\n",
    "    wday_mat = df_enf[wday_cols]\n",
    "    wday_sum = wday_mat.sum(axis=1)\n",
    "    mask_multi = wday_sum > 1.0\n",
    "    if mask_multi.any():\n",
    "        idx = df_enf.index[mask_multi]\n",
    "        argmax = wday_mat.loc[idx].values.argmax(axis=1)\n",
    "        names = np.array(wday_cols)\n",
    "        chosen = names[argmax]\n",
    "        df_enf.loc[idx, wday_cols] = 0.0\n",
    "        for i, r in enumerate(idx):\n",
    "            df_enf.loc[r, chosen[i]] = 1.0\n",
    "        add_fix(\"weekday exclusivity repaired (multi→single 1)\", int(mask_multi.sum()))\n",
    "\n",
    "# Channel exclusivity: a lo más un 1, mismo criterio\n",
    "chan_cols = [c for c in df_enf.columns if c.startswith(\"data_channel_is_\")]\n",
    "if len(chan_cols) >= 2:\n",
    "    chan_mat = df_enf[chan_cols]\n",
    "    chan_sum = chan_mat.sum(axis=1)\n",
    "    mask_multi = chan_sum > 1.0\n",
    "    if mask_multi.any():\n",
    "        idx = df_enf.index[mask_multi]\n",
    "        argmax = chan_mat.loc[idx].values.argmax(axis=1)\n",
    "        names = np.array(chan_cols)\n",
    "        chosen = names[argmax]\n",
    "        df_enf.loc[idx, chan_cols] = 0.0\n",
    "        for i, r in enumerate(idx):\n",
    "            df_enf.loc[r, chosen[i]] = 1.0\n",
    "        add_fix(\"channel exclusivity repaired (multi→single 1)\", int(mask_multi.sum()))\n",
    "\n",
    "# Consistencia de fin de semana\n",
    "if {\"is_weekend\", \"weekday_is_saturday\", \"weekday_is_sunday\"}.issubset(df_enf.columns):\n",
    "    expected = ((df_enf[\"weekday_is_saturday\"] == 1.0) | (df_enf[\"weekday_is_sunday\"] == 1.0)).astype(float)\n",
    "    mask = expected.notna()\n",
    "    before = df_enf.loc[mask, \"is_weekend\"].copy()\n",
    "    df_enf.loc[mask, \"is_weekend\"] = expected[mask]\n",
    "    n = (before != df_enf.loc[mask, \"is_weekend\"]).sum()\n",
    "    add_fix(\"is_weekend aligned with Saturday/Sunday\", int(n))\n",
    "\n",
    "# ---------------------------------\n",
    "# 6) Counts/tokens: no negativos\n",
    "# ---------------------------------\n",
    "count_cols = [c for c in df_enf.columns if c.startswith(\"n_tokens_\")] + \\\n",
    "             [c for c in df_enf.columns if c.startswith(\"num_\")] + [\"num_keywords\", \"average_token_length\"]\n",
    "for c in count_cols:\n",
    "    if c in df_enf.columns:\n",
    "        s = df_enf[c]\n",
    "        mask_neg = s.notna() & (s < 0)\n",
    "        n = mask_neg.sum()\n",
    "        df_enf.loc[mask_neg, c] = np.nan\n",
    "        add_fix(f\"{c}: negatives invalid -> NaN\", n)\n",
    "\n",
    "# ---------------------------------\n",
    "# Finalizar Step 3\n",
    "# ---------------------------------\n",
    "df_clean_stage3 = df_enf.copy()\n",
    "\n",
    "print(\"Step 3 — Enforcement complete. Sample of applied fixes:\")\n",
    "for msg, n in fix_report[:]:\n",
    "    print(f\" - {msg}: {n}\")\n",
    "print(f\"\\nShape after enforcement: {df_clean_stage3.shape[0]} rows × {df_clean_stage3.shape[1]} cols\")\n",
    "print(\"Ready for Step 4 (Imputation and record handling).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "K_mFHBvTb7Fc",
    "papermill": {
     "duration": 0.0127,
     "end_time": "2025-10-12T21:17:21.213365",
     "exception": false,
     "start_time": "2025-10-12T21:17:21.200665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Enforcement results and transition to Step 4\n",
    "\n",
    "The enforcement phase successfully corrected all structural and logical inconsistencies identified in the grouped diagnostics.  \n",
    "No rows were dropped; instead, invalid or sentinel values were replaced with `NaN`, and all variables now conform to their logical ranges and dependencies.\n",
    "\n",
    "### Key outcomes\n",
    "\n",
    "- **Dataset shape:** 39,314 rows × 62 columns  \n",
    "- **`timedelta`** out of [8,731] → set to NaN: 312 rows  \n",
    "- **Keyword features (`kw_min_min`, `kw_avg_min`, `kw_min_avg`)** negatives treated as missing: 23,000+ values converted to NaN  \n",
    "- **LDA topics:** 1,778 out-of-range entries corrected; all 39,313 rows renormalized to sum ≈ 1  \n",
    "- **Polarity & subjectivity:** thousands of values out of expected domains replaced with NaN  \n",
    "  - Positive polarities → [0, 1]  \n",
    "  - Negative polarities → [−1, 0]  \n",
    "  - Global/title polarities → [−1, 1]  \n",
    "- **`abs_title_sentiment_polarity`** corrected to `abs(title_sentiment_polarity)` where possible (269 fixes, 579 fills)  \n",
    "- **Binary flags:** snapped to {0, 1} and repaired for exclusivity and weekend alignment (973 rows adjusted)\n",
    "\n",
    "At this point, the dataset is internally consistent and all remaining NaNs represent true missing or irreparable data.\n",
    "\n",
    "### Next step: Imputation and record handling\n",
    "\n",
    "We will:\n",
    "- Quantify the remaining missingness per column and per record.\n",
    "- Apply targeted imputation strategies (mean/median/mode or logical filling).\n",
    "- Optionally drop rows with excessive NaNs or corrupted values.\n",
    "- Produce the finalized, model-ready dataset `df_ready` for Step 5 (Validation & Export).\n",
    "\n",
    "The enforcement step marks the transition from **structural cleaning** to **statistical repair**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "Tr0uDIVgjlZk",
    "papermill": {
     "duration": 0.013818,
     "end_time": "2025-10-12T21:17:21.238120",
     "exception": false,
     "start_time": "2025-10-12T21:17:21.224302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 4. Imputation and record handling: goals and plan\n",
    "\n",
    "With structural cleaning and logical enforcement complete, this step focuses on statistical repair. We will:\n",
    "1) Quantify remaining missingness at column and row level, and verify that no out-of-domain values remain after enforcement.\n",
    "2) Summarize missingness by feature group to drive group-specific imputation decisions.\n",
    "3) Define row handling policies, for example dropping a small fraction of records with excessive missingness that would bias imputations.\n",
    "4) Propose imputation strategies aligned with feature semantics:\n",
    "   • Continuous metrics and counts, median or robust statistics.  \n",
    "   • Binary flags, mode and logical reconstruction when applicable.  \n",
    "   • LDA topics, fill only where there is partial evidence, then re-normalize per row.  \n",
    "   • Sentiment and subjectivity, bounded replacements, typically median.  \n",
    "   • Special keyword sentinels already mapped to NaN, impute if informative or leave for model that supports missingness.\n",
    "This notebook section will produce clear diagnostics and a draft policy that we will implement in the next substep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 791,
     "status": "ok",
     "timestamp": 1760206650106,
     "user": {
      "displayName": "Felipe de Jesús Gutiérrez Dávila",
      "userId": "01500989835494077702"
     },
     "user_tz": 360
    },
    "id": "yE3NezczZmKD",
    "outputId": "49338f58-141d-4f0d-bdaa-47f60424373f",
    "papermill": {
     "duration": 0.584116,
     "end_time": "2025-10-12T21:17:21.833033",
     "exception": false,
     "start_time": "2025-10-12T21:17:21.248917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 4.1 — Missingness and readiness diagnostics (no imputation yet)\n",
    "# Requires: df_clean_stage3 from Step 3\n",
    "# ============================================================\n",
    "\n",
    "assert \"df_clean_stage3\" in globals(), \"df_clean_stage3 not found. Run Step 3 enforcement first.\"\n",
    "df4 = df_clean_stage3.copy()\n",
    "\n",
    "print(f\"Working shape for Step 4: {df4.shape[0]} rows × {df4.shape[1]} columns\\n\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) Define groups again to report by feature family\n",
    "# -------------------------------------------------\n",
    "cols = df4.columns.tolist()\n",
    "\n",
    "lda_cols      = [c for c in cols if c.startswith(\"LDA_\")]\n",
    "chan_cols     = [c for c in cols if c.startswith(\"data_channel_is_\")]\n",
    "wday_cols     = [c for c in cols if c.startswith(\"weekday_is_\")]\n",
    "rate_cols     = [c for c in cols if c in [\n",
    "    \"global_rate_positive_words\", \"global_rate_negative_words\",\n",
    "    \"rate_positive_words\", \"rate_negative_words\"\n",
    "]]\n",
    "subj_cols     = [c for c in cols if c in [\n",
    "    \"global_subjectivity\", \"title_subjectivity\", \"abs_title_subjectivity\"\n",
    "]]\n",
    "pol_pos_cols  = [c for c in cols if c in [\n",
    "    \"avg_positive_polarity\", \"min_positive_polarity\", \"max_positive_polarity\"\n",
    "]]\n",
    "pol_neg_cols  = [c for c in cols if c in [\n",
    "    \"avg_negative_polarity\", \"min_negative_polarity\", \"max_negative_polarity\"\n",
    "]]\n",
    "pol_any_cols  = [c for c in cols if c in [\n",
    "    \"global_sentiment_polarity\", \"title_sentiment_polarity\"\n",
    "]]\n",
    "kw_cols       = [c for c in cols if c.startswith(\"kw_\")]\n",
    "selfref_cols  = [c for c in cols if c.startswith(\"self_reference_\")]\n",
    "count_cols    = [c for c in cols if c.startswith(\"n_tokens_\")] + \\\n",
    "                [c for c in cols if c.startswith(\"num_\")] + [\"num_keywords\", \"average_token_length\"]\n",
    "bin_cols      = list(sorted(set(chan_cols + wday_cols + [\"is_weekend\"]).intersection(cols)))\n",
    "\n",
    "exclude = set([\"url\"] + lda_cols + chan_cols + wday_cols + rate_cols + subj_cols +\n",
    "              pol_pos_cols + pol_neg_cols + pol_any_cols + kw_cols + selfref_cols +\n",
    "              count_cols + bin_cols)\n",
    "cont_cols = [c for c in cols if c not in exclude]  # remaining continuous-like features\n",
    "groups = {\n",
    "    \"LDA\": lda_cols,\n",
    "    \"Channels (binary)\": chan_cols,\n",
    "    \"Weekdays (binary)\": wday_cols,\n",
    "    \"Rates\": rate_cols,\n",
    "    \"Subjectivity\": subj_cols,\n",
    "    \"Positive polarities\": pol_pos_cols,\n",
    "    \"Negative polarities\": pol_neg_cols,\n",
    "    \"Global/Title polarities\": pol_any_cols,\n",
    "    \"Keyword stats (kw_*)\": kw_cols,\n",
    "    \"Self-reference\": selfref_cols,\n",
    "    \"Counts/tokens\": count_cols,\n",
    "    \"Other continuous\": cont_cols,\n",
    "}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) Column-level missingness summary and top offenders\n",
    "# -------------------------------------------------\n",
    "col_na = df4.isna().sum().sort_values(ascending=False)\n",
    "col_na_pct = (col_na / len(df4) * 100).round(3)\n",
    "\n",
    "print(\"Top 20 columns by missing percentage:\\n\")\n",
    "print(pd.DataFrame({\"missing\": col_na.head(20), \"missing_pct\": col_na_pct.head(20)}))\n",
    "\n",
    "# Simple bar plot for top missing columns\n",
    "top_k = 20\n",
    "plt.figure(figsize=(10, 5))\n",
    "col_na_pct.head(top_k).iloc[::-1].plot(kind=\"barh\")\n",
    "plt.title(f\"Top {top_k} columns by missing percentage\")\n",
    "plt.xlabel(\"Percent missing\")\n",
    "plt.ylabel(\"Column\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Row-level missingness distribution\n",
    "# -------------------------------------------------\n",
    "row_na_counts = df4.isna().sum(axis=1)\n",
    "row_na_pct = (row_na_counts / df4.shape[1]) * 100\n",
    "\n",
    "print(\"\\nRow missingness summary:\")\n",
    "print(row_na_counts.describe(percentiles=[0.5, 0.9, 0.95, 0.99]).round(2))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(row_na_counts, bins=40)\n",
    "plt.title(\"Distribution of missing values per row\")\n",
    "plt.xlabel(\"Number of missing features in row\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(row_na_pct, bins=40)\n",
    "plt.title(\"Distribution of missing percentage per row\")\n",
    "plt.xlabel(\"Percent of features missing in row\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) Missingness by group, plus quick validity checks\n",
    "# -------------------------------------------------\n",
    "def group_missing_report(df, group_dict):\n",
    "    rows = []\n",
    "    for gname, glist in group_dict.items():\n",
    "        glist = [c for c in glist if c in df.columns]\n",
    "        if not glist:\n",
    "            continue\n",
    "        na_counts = df[glist].isna().sum()\n",
    "        na_pct = (na_counts / len(df) * 100)\n",
    "        rows.append({\n",
    "            \"group\": gname,\n",
    "            \"n_cols\": len(glist),\n",
    "            \"avg_missing_pct\": na_pct.mean(),\n",
    "            \"max_missing_pct\": na_pct.max(),\n",
    "            \"cols_over_5pct\": int((na_pct > 5).sum()),\n",
    "        })\n",
    "    rep = pd.DataFrame(rows).sort_values(by=[\"avg_missing_pct\", \"max_missing_pct\"], ascending=False)\n",
    "    return rep\n",
    "\n",
    "group_rep = group_missing_report(df4, groups)\n",
    "print(\"\\nMissingness by group:\")\n",
    "print(group_rep.to_string(index=False, formatters={\n",
    "    \"avg_missing_pct\": lambda x: f\"{x:0.2f}\",\n",
    "    \"max_missing_pct\": lambda x: f\"{x:0.2f}\",\n",
    "}))\n",
    "\n",
    "# Quick residual validity checks after enforcement, defensive only\n",
    "def check_bounds(df, cols, lo, hi):\n",
    "    bad = 0\n",
    "    for c in cols:\n",
    "        if c in df:\n",
    "            s = df[c]\n",
    "            bad += int((((s < lo) | (s > hi)) & s.notna()).sum())\n",
    "    return bad\n",
    "\n",
    "residual = {\n",
    "    \"LDA_out_of_[0,1]\": check_bounds(df4, lda_cols, 0.0, 1.0),\n",
    "    \"Rates_out_of_[0,1]\": check_bounds(df4, rate_cols, 0.0, 1.0) + check_bounds(df4, subj_cols, 0.0, 1.0),\n",
    "    \"Pos_polarity_out_of_[0,1]\": check_bounds(df4, pol_pos_cols, 0.0, 1.0),\n",
    "    \"Neg_polarity_out_of_[-1,0]\": check_bounds(df4, pol_neg_cols, -1.0, 0.0),\n",
    "    \"GlobalTitle_polarity_out_of_[-1,1]\": check_bounds(df4, pol_any_cols, -1.0, 1.0),\n",
    "}\n",
    "print(\"\\nResidual out-of-domain counts after enforcement (should be zeros):\")\n",
    "for k, v in residual.items():\n",
    "    print(f\" - {k}: {v}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) Draft imputation policy outline (no changes made yet)\n",
    "# -------------------------------------------------\n",
    "imputation_policy = {\n",
    "    \"continuous\": {\n",
    "        \"columns\": sorted(set(count_cols + selfref_cols + cont_cols + [c for c in kw_cols if c.startswith(\"kw_\")])),\n",
    "        \"strategy\": \"median, with optional winsorization for heavy tails\",\n",
    "    },\n",
    "    \"binary\": {\n",
    "        \"columns\": sorted(set(bin_cols)),\n",
    "        \"strategy\": \"mode with logical reconstruction where applicable (weekday exclusivity, weekend alignment)\",\n",
    "    },\n",
    "    \"lda\": {\n",
    "        \"columns\": sorted(set(lda_cols)),\n",
    "        \"strategy\": \"if partial topics present, fill missing with column median or small epsilon, then renormalize row; if all missing, leave NaN for downstream handling\",\n",
    "    },\n",
    "    \"sentiment_subjectivity\": {\n",
    "        \"columns\": sorted(set(rate_cols + subj_cols + pol_pos_cols + pol_neg_cols + pol_any_cols)),\n",
    "        \"strategy\": \"median within valid bounds; preserve sign constraints for negative polarities\",\n",
    "    },\n",
    "    \"non_predictive\": {\n",
    "        \"columns\": [\"url\", \"timedelta\"],\n",
    "        \"strategy\": \"no imputation for url; optional median for timedelta if needed for analyses\",\n",
    "    },\n",
    "    \"row_policy\": \"consider dropping rows with >20% missing features after group imputations, and always document counts\",\n",
    "}\n",
    "\n",
    "print(\"\\nDraft imputation policy (summary, no changes applied):\")\n",
    "for k, v in imputation_policy.items():\n",
    "    print(f\"- {k}: {v if isinstance(v, str) else 'ok'}\")\n",
    "\n",
    "# Keep a copy for the next substep\n",
    "df4_stageA = df4.copy()\n",
    "print(\"\\nDiagnostics complete. Ready to implement Step 4.2 (actual imputation) based on the above policy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "wAmb3rMhnumc",
    "papermill": {
     "duration": 0.013533,
     "end_time": "2025-10-12T21:17:21.859729",
     "exception": false,
     "start_time": "2025-10-12T21:17:21.846196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary — Missingness and readiness diagnostics\n",
    "\n",
    "After the enforcement step, our dataset remains **clean and well-structured**, with only moderate missingness mainly concentrated in two families of variables:\n",
    "\n",
    "- **Keyword stats (`kw_*`)** show the largest gaps, notably `kw_min_min` (≈ 58 % NaN) and `kw_avg_min` (≈ 3.6 %).  \n",
    "  These negatives had been flagged earlier as sentinel values; now correctly treated as missing.\n",
    "- **Other continuous features** include a single variable with ≈ 30 % missing (`mixed_type_col`), likely injected noise.  \n",
    "- All other groups (LDA, polarities, subjectivity, rates, channels, weekdays, etc.) remain extremely stable, with **≈ 2–2.5 % missing**, well within manageable limits.\n",
    "- **Row-level missingness:**  \n",
    "  - Median = 2 missing features per row  \n",
    "  - 95th percentile = 4  \n",
    "  - Maximum = 8  \n",
    "  → No extreme sparsity, so full-row deletion is unnecessary.\n",
    "- **Residual out-of-range values:** All zeros, the enforcement step fully repaired domain consistency.\n",
    "\n",
    "Overall, the dataset is now *structurally valid* and *statistically ready* for imputation.  \n",
    "Only a few columns require explicit filling; others can safely keep small NaN fractions.\n",
    "\n",
    "---\n",
    "\n",
    "### Next step: Step 4.2 — Group-wise imputation\n",
    "\n",
    "We will now:\n",
    "1. Apply **median/mode imputation** depending on variable type.  \n",
    "2. For LDA topics, fill partial NaN values then renormalize row-wise to preserve ∑ = 1.  \n",
    "3. For sentiment and subjectivity variables, fill with group medians inside valid bounds.  \n",
    "4. For binary weekday/channel flags, use mode and re-enforce logical relations (`weekday_exclusivity` and `weekend_consistency`).  \n",
    "5. Leave highly corrupted columns (`kw_min_min`, `mixed_type_col`) as candidates for later exclusion if they remain uninformative.\n",
    "\n",
    "The resulting `df_clean_stage4` will represent the first **fully analytical-ready dataset**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 151,
     "status": "ok",
     "timestamp": 1760206650258,
     "user": {
      "displayName": "Felipe de Jesús Gutiérrez Dávila",
      "userId": "01500989835494077702"
     },
     "user_tz": 360
    },
    "id": "wrZQs27jkREO",
    "outputId": "c4b0b818-1b72-44b2-e333-013684edeb39",
    "papermill": {
     "duration": 0.183381,
     "end_time": "2025-10-12T21:17:22.055326",
     "exception": false,
     "start_time": "2025-10-12T21:17:21.871945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 4.2 — Group-wise Imputation according to the draft policy\n",
    "# ============================================================\n",
    "\n",
    "df5 = df4_stageA.copy()\n",
    "\n",
    "# ------------------------------\n",
    "# Continuous and count features\n",
    "# ------------------------------\n",
    "cont_targets = imputation_policy[\"continuous\"][\"columns\"]\n",
    "for c in cont_targets:\n",
    "    if c in df5:\n",
    "        med = df5[c].median(skipna=True)\n",
    "        df5[c] = df5[c].fillna(med)\n",
    "\n",
    "# ------------------------------\n",
    "# Binary features\n",
    "# ------------------------------\n",
    "bin_targets = imputation_policy[\"binary\"][\"columns\"]\n",
    "for c in bin_targets:\n",
    "    if c in df5:\n",
    "        mode_val = df5[c].mode(dropna=True)\n",
    "        if not mode_val.empty:\n",
    "            df5[c] = df5[c].fillna(mode_val.iloc[0])\n",
    "\n",
    "# Logical consistency recheck for weekend alignment\n",
    "if {\"weekday_is_saturday\", \"weekday_is_sunday\", \"is_weekend\"}.issubset(df5.columns):\n",
    "    sat_sun = df5[\"weekday_is_saturday\"].fillna(0) + df5[\"weekday_is_sunday\"].fillna(0)\n",
    "    df5.loc[sat_sun > 0, \"is_weekend\"] = 1\n",
    "    df5.loc[sat_sun == 0, \"is_weekend\"] = 0\n",
    "\n",
    "# ------------------------------\n",
    "# LDA topics: fill partials + renormalize\n",
    "# ------------------------------\n",
    "lda_targets = imputation_policy[\"lda\"][\"columns\"]\n",
    "lda_df = df5[lda_targets].copy()\n",
    "\n",
    "lda_missing = lda_df.isna().sum().sum()\n",
    "if lda_missing > 0:\n",
    "    col_meds = lda_df.median(skipna=True)\n",
    "    lda_df = lda_df.fillna(col_meds)\n",
    "    lda_df_sum = lda_df.sum(axis=1)\n",
    "    lda_df = lda_df.div(lda_df_sum, axis=0)\n",
    "    df5[lda_targets] = lda_df\n",
    "    print(f\"LDA topics imputed & renormalized — {lda_missing} values filled.\")\n",
    "\n",
    "# ------------------------------\n",
    "# Sentiment & subjectivity\n",
    "# ------------------------------\n",
    "sent_targets = imputation_policy[\"sentiment_subjectivity\"][\"columns\"]\n",
    "for c in sent_targets:\n",
    "    if c in df5:\n",
    "        med = df5[c].median(skipna=True)\n",
    "        df5[c] = df5[c].fillna(med)\n",
    "        # enforce valid bounds\n",
    "        if \"negative\" in c:\n",
    "            df5[c] = df5[c].clip(-1, 0)\n",
    "        elif \"positive\" in c or \"subjectivity\" in c or \"rate\" in c:\n",
    "            df5[c] = df5[c].clip(0, 1)\n",
    "        elif \"polarity\" in c:\n",
    "            df5[c] = df5[c].clip(-1, 1)\n",
    "\n",
    "# ------------------------------\n",
    "# Final NaN check\n",
    "# ------------------------------\n",
    "total_missing = df5.isna().sum().sum()\n",
    "print(f\"\\nTotal remaining missing values after imputation: {total_missing}\")\n",
    "\n",
    "if total_missing > 0:\n",
    "    rows_heavy_na = (df5.isna().sum(axis=1) > 0.2 * df5.shape[1]).sum()\n",
    "    print(f\"Rows exceeding 20% missingness (before potential drop): {rows_heavy_na}\")\n",
    "\n",
    "# Export to next stage\n",
    "df_clean_stage4 = df5.copy()\n",
    "print(f\"\\nStep 4.2 complete — ready for Step 5 (Validation & Export). Shape: {df_clean_stage4.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "2I10mO-frY4V",
    "papermill": {
     "duration": 0.013639,
     "end_time": "2025-10-12T21:17:22.082311",
     "exception": false,
     "start_time": "2025-10-12T21:17:22.068672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 5. Final Validation and Export\n",
    "\n",
    "After the grouped imputation (Step 4), our dataset reaches a **fully analytical-ready** state:\n",
    "\n",
    "- **Integrity:** Only 6 residual NaNs remain across 62 columns (< 0.0003 %), a negligible fraction.\n",
    "- **No structural loss:** Row count preserved at 39,314 after deduplication and cleaning.\n",
    "- **LDA consistency:** All 5 topic probabilities now in [0, 1] and ∑ ≈ 1 per row.\n",
    "- **Bound compliance:** No polarity, rate, or subjectivity variable exceeds its theoretical domain.\n",
    "- **Binary exclusivity:** Channels and weekdays satisfy their logical constraints, weekend flag aligned.\n",
    "\n",
    "At this stage, the dataset can be confidently used for modeling and EDA.  \n",
    "However, before exporting, we’ll perform **final validation** to quantify how much transformation occurred compared with the *original dataset*.\n",
    "\n",
    "### Validation goals:\n",
    "1. Compare descriptive statistics between original and final datasets.  \n",
    "2. Verify bounds, missingness, and basic distribution stability.  \n",
    "3. Confirm that corrections did not distort target-related columns like `shares`.  \n",
    "4. Export the final cleaned DataFrame (`df_clean_final.csv`) for downstream analysis.\n",
    "\n",
    "This marks the completion of the preprocessing pipeline — a reproducible and well-documented cleaning workflow from raw data to a high-quality analytical dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3169,
     "status": "ok",
     "timestamp": 1760206653429,
     "user": {
      "displayName": "Felipe de Jesús Gutiérrez Dávila",
      "userId": "01500989835494077702"
     },
     "user_tz": 360
    },
    "id": "COx3zCgzoyV7",
    "outputId": "0bc360a6-893d-4424-b55a-927ad9aced20",
    "papermill": {
     "duration": 3.72848,
     "end_time": "2025-10-12T21:17:25.825164",
     "exception": false,
     "start_time": "2025-10-12T21:17:22.096684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 5 — Validation and Export\n",
    "# ============================================================\n",
    "\n",
    "# Cargar el dataset original\n",
    "#df_orig = pd.read_csv(path_original)\n",
    "\n",
    "# ============================================================\n",
    "# Step 5 — Validation and Export\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Cargar el dataset original (desde el repo, no Colab)\n",
    "path_original = Path(\"data/raw/online_news_original.csv\")\n",
    "if path_original.exists():\n",
    "    df_orig = pd.read_csv(path_original, engine=\"python\")\n",
    "    HAVE_ORIG = True\n",
    "else:\n",
    "    print(\"⚠️ No se encontró data/raw/online_news_original.csv — se omite la comparación con el original.\")\n",
    "    HAVE_ORIG = False\n",
    "\n",
    "\n",
    "# --- 0) Rebuild column groups to ensure context\n",
    "cols = df_clean_stage4.columns.tolist()\n",
    "\n",
    "lda_cols   = [c for c in cols if c.startswith(\"LDA_\")]\n",
    "chan_cols  = [c for c in cols if c.startswith(\"data_channel_is_\")]\n",
    "wday_cols  = [c for c in cols if c.startswith(\"weekday_is_\")]\n",
    "rate_cols  = [c for c in cols if c in [\"global_rate_positive_words\", \"global_rate_negative_words\",\n",
    "                                       \"rate_positive_words\", \"rate_negative_words\"]]\n",
    "subj_cols  = [c for c in cols if c in [\"global_subjectivity\", \"title_subjectivity\", \"abs_title_subjectivity\"]]\n",
    "pol_cols   = [c for c in cols if c in [\"global_sentiment_polarity\", \"title_sentiment_polarity\",\n",
    "                                       \"avg_positive_polarity\", \"min_positive_polarity\", \"max_positive_polarity\",\n",
    "                                       \"avg_negative_polarity\", \"min_negative_polarity\", \"max_negative_polarity\",\n",
    "                                       \"abs_title_sentiment_polarity\"]]\n",
    "bin_cols   = sorted(set(chan_cols + wday_cols + [\"is_weekend\"]).intersection(cols))\n",
    "\n",
    "# --- 1) Structural validation\n",
    "print(\"Structural validation:\")\n",
    "print(f\"Original shape: {df_orig.shape}\")\n",
    "print(f\"Modified shape: {df_clean_stage4.shape}\\n\")\n",
    "\n",
    "# --- 2) Global missingness comparison\n",
    "miss_orig = df_orig.isna().sum().sum()\n",
    "miss_final = df_clean_stage4.isna().sum().sum()\n",
    "print(f\"Total missing values — original: {miss_orig}, final: {miss_final}\")\n",
    "\n",
    "# --- 3) Compare descriptive statistics (sample)\n",
    "def summarize(df, cols, name):\n",
    "    print(f\"\\nSummary for {name}:\")\n",
    "    display(df[cols].describe().T.iloc[:, :6])\n",
    "\n",
    "numeric_cols = [c for c in df_clean_stage4.columns if c != \"url\"]\n",
    "summarize(df_orig, [c for c in numeric_cols if c in df_orig.columns][:10], \"original (sample)\")\n",
    "summarize(df_clean_stage4, numeric_cols[:10], \"cleaned (sample)\")\n",
    "\n",
    "# --- 4) Validate domain bounds\n",
    "def validate_bounds(df):\n",
    "    issues = []\n",
    "    # LDA sum ≈ 1\n",
    "    lda_sum = df[lda_cols].sum(axis=1)\n",
    "    if not ((lda_sum.between(0.95, 1.05)).all()):\n",
    "        issues.append(\"LDA sum outside [0.95,1.05]\")\n",
    "    # Rates & subjectivities\n",
    "    for c in rate_cols + subj_cols:\n",
    "        if not df[c].between(0, 1).all():\n",
    "            issues.append(f\"{c} outside [0,1]\")\n",
    "    # Polarities\n",
    "    for c in pol_cols:\n",
    "        if not df[c].between(-1, 1).all():\n",
    "            issues.append(f\"{c} outside [-1,1]\")\n",
    "    # Binary flags\n",
    "    for c in bin_cols:\n",
    "        if not df[c].isin([0, 1]).all():\n",
    "            issues.append(f\"{c} not binary\")\n",
    "    return issues\n",
    "\n",
    "issues = validate_bounds(df_clean_stage4)\n",
    "if len(issues) == 0:\n",
    "    print(\"\\nAll validation checks passed — dataset is consistent and bounded.\")\n",
    "else:\n",
    "    print(\"\\nValidation issues found:\")\n",
    "    for i in issues:\n",
    "        print(\"-\", i)\n",
    "\n",
    "# --- 5) Target variable sanity check\n",
    "if \"shares\" in df_clean_stage4.columns:\n",
    "    print(\"\\nTarget variable 'shares' summary (before vs after cleaning):\")\n",
    "    comp = pd.DataFrame({\n",
    "        \"original\": df_orig[\"shares\"].describe(),\n",
    "        \"cleaned\": df_clean_stage4[\"shares\"].describe()\n",
    "    })\n",
    "    display(comp)\n",
    "\n",
    "# --- 6) Export final dataset\n",
    "output_path = \"df_clean_final.csv\"\n",
    "df_clean_stage4.to_csv(output_path, index=False)\n",
    "print(f\"\\nFinal dataset exported to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "THG4ZHlXynX9",
    "papermill": {
     "duration": 0.014166,
     "end_time": "2025-10-12T21:17:25.853383",
     "exception": false,
     "start_time": "2025-10-12T21:17:25.839217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 5.2 — Statistical reconstruction toward the original distribution\n",
    "\n",
    "Goal: adjust the most distorted features in the **modified + cleaned** dataset so their range and scale better approximate the **original dataset**. We will:\n",
    "1) Use the **original dataset’s** mean and standard deviation as a *canonical reference*.\n",
    "2) Apply **Z-score clipping** (μ ± 3σ from the original) on key families that show inflated spread:  \n",
    "   • token/count features (`n_tokens_*`, `num_*`)  \n",
    "   • resource counts (`num_hrefs`, `num_imgs`, `num_videos`, `num_self_hrefs`)  \n",
    "   • target (`shares`)  \n",
    "   • selected `kw_*` metrics as needed  \n",
    "3) Enforce [0,1] for features that are conceptually proportions when applicable (e.g., `n_unique_tokens`, `n_non_stop_*`), without reintroducing NaNs.\n",
    "4) Validate with overlayed histograms (original vs reconstructed) on representative variables.\n",
    "5) Re-check constraints and export `df_reconstructed.csv`.\n",
    "\n",
    "Note: This is **reconstruction**, not re-cleaning. No mass row drops, and no additional NaN creation. We only curb extremes to recover realistic shape and scale relative to the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5655,
     "status": "ok",
     "timestamp": 1760206659110,
     "user": {
      "displayName": "Felipe de Jesús Gutiérrez Dávila",
      "userId": "01500989835494077702"
     },
     "user_tz": 360
    },
    "id": "mGJQZDYptJSk",
    "outputId": "f4db876f-296e-44eb-90a3-d9f3a2f625b8",
    "papermill": {
     "duration": 3.207719,
     "end_time": "2025-10-12T21:17:29.073990",
     "exception": false,
     "start_time": "2025-10-12T21:17:25.866271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 5.2 — Statistical Reconstruction toward Original Distribution\n",
    "# Requires: df_orig (original UCI dataset) and df_clean_stage4 (cleaned modified dataset)\n",
    "# Produces: df_reconstructed\n",
    "# ============================================================\n",
    "\n",
    "# 0) Base copies\n",
    "df_recon = df_clean_stage4.copy()\n",
    "print(f\"Starting reconstruction with shape: {df_recon.shape}\")\n",
    "\n",
    "# 1) Select columns to reconstruct (heavily distorted families)\n",
    "tokens_counts = [\n",
    "    \"n_tokens_title\", \"n_tokens_content\",\n",
    "    \"n_unique_tokens\", \"n_non_stop_words\", \"n_non_stop_unique_tokens\",\n",
    "    \"num_hrefs\", \"num_self_hrefs\", \"num_imgs\", \"num_videos\"\n",
    "]\n",
    "target_cols = [\"shares\"]\n",
    "\n",
    "# Optional: include additional kw_* that show heavy tails after cleaning\n",
    "kw_focus = [\"kw_min_min\", \"kw_avg_min\", \"kw_min_avg\"]\n",
    "cols_to_clip = [c for c in tokens_counts + target_cols + kw_focus if c in df_recon.columns and c in df_orig.columns]\n",
    "\n",
    "# 2) Z-score clipping using original distribution (μ, σ) as canonical reference\n",
    "def zscore_clip_to_original(df_ref, df_to_fix, columns, k=3.0):\n",
    "    applied = []\n",
    "    for c in columns:\n",
    "        mu = df_ref[c].mean()\n",
    "        sigma = df_ref[c].std()\n",
    "        if pd.isna(mu) or pd.isna(sigma) or sigma == 0:\n",
    "            # fallback to percentiles if σ is degenerate\n",
    "            low = df_ref[c].quantile(0.01)\n",
    "            high = df_ref[c].quantile(0.99)\n",
    "            df_to_fix[c] = df_to_fix[c].clip(lower=low, upper=high)\n",
    "            applied.append((c, f\"percentile_clip[1%,99%] fallback (σ={sigma})\"))\n",
    "        else:\n",
    "            low = mu - k * sigma\n",
    "            high = mu + k * sigma\n",
    "            before_outliers = int(((df_to_fix[c] < low) | (df_to_fix[c] > high)).sum())\n",
    "            df_to_fix[c] = df_to_fix[c].clip(lower=low, upper=high)\n",
    "            after_outliers = int(((df_to_fix[c] < low) | (df_to_fix[c] > high)).sum())\n",
    "            applied.append((c, f\"zclip[{low:0.3f},{high:0.3f}] cut={before_outliers-after_outliers}\"))\n",
    "    return df_to_fix, applied\n",
    "\n",
    "df_recon, applied_info = zscore_clip_to_original(df_orig, df_recon, cols_to_clip, k=3.0)\n",
    "print(\"\\nZ-score clipping summary (original μ±3σ reference):\")\n",
    "for c, msg in applied_info:\n",
    "    print(f\" - {c}: {msg}\")\n",
    "\n",
    "# 3) Enforce [0,1] for proportion-like columns, without creating NaNs\n",
    "proportion_like = [c for c in [\"n_unique_tokens\", \"n_non_stop_words\", \"n_non_stop_unique_tokens\"] if c in df_recon.columns]\n",
    "for c in proportion_like:\n",
    "    df_recon[c] = df_recon[c].clip(0, 1)\n",
    "\n",
    "# 4) Quick comparative summaries (original vs reconstructed) for key variables\n",
    "def compare_stats(ref, new, columns, title):\n",
    "    rows = []\n",
    "    for c in columns:\n",
    "        if c in ref.columns and c in new.columns:\n",
    "            rows.append({\n",
    "                \"column\": c,\n",
    "                \"orig_mean\": ref[c].mean(), \"recon_mean\": new[c].mean(),\n",
    "                \"orig_std\": ref[c].std(),   \"recon_std\": new[c].std(),\n",
    "                \"orig_min\": ref[c].min(),   \"recon_min\": new[c].min(),\n",
    "                \"orig_p50\": ref[c].median(),\"recon_p50\": new[c].median(),\n",
    "                \"orig_p95\": ref[c].quantile(0.95), \"recon_p95\": new[c].quantile(0.95),\n",
    "                \"orig_max\": ref[c].max(),   \"recon_max\": new[c].max()\n",
    "            })\n",
    "    rep = pd.DataFrame(rows)\n",
    "    print(f\"\\n{title}\")\n",
    "    display(rep)\n",
    "\n",
    "compare_stats(df_orig, df_recon, [\"shares\", \"n_tokens_content\", \"num_imgs\", \"n_unique_tokens\"],\n",
    "              \"Comparison of key variables (original vs reconstructed)\")\n",
    "\n",
    "# 5) Visual validation: overlay histograms (no specific colors)\n",
    "vars_to_plot = [\"shares\", \"n_tokens_content\", \"num_imgs\", \"n_unique_tokens\"]\n",
    "for var in vars_to_plot:\n",
    "    if var in df_orig.columns and var in df_recon.columns:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.hist(df_orig[var].dropna(), bins=50, alpha=0.5, label=\"Original\")\n",
    "        plt.hist(df_recon[var].dropna(), bins=50, alpha=0.5, label=\"Reconstructed\")\n",
    "        plt.title(f\"{var}: original vs reconstructed\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 6) Constraint re-checks (defensive)\n",
    "def lda_sum_ok(df, lda_cols):\n",
    "    s = df[lda_cols].sum(axis=1)\n",
    "    return bool((s.between(0.95, 1.05)).all())\n",
    "\n",
    "lda_cols = [c for c in df_recon.columns if c.startswith(\"LDA_\")]\n",
    "pol_pos_cols = [c for c in df_recon.columns if c in [\"avg_positive_polarity\",\"min_positive_polarity\",\"max_positive_polarity\"]]\n",
    "pol_neg_cols = [c for c in df_recon.columns if c in [\"avg_negative_polarity\",\"min_negative_polarity\",\"max_negative_polarity\"]]\n",
    "pol_any_cols = [c for c in df_recon.columns if c in [\"global_sentiment_polarity\",\"title_sentiment_polarity\"]]\n",
    "rate_cols = [c for c in df_recon.columns if c in [\"global_rate_positive_words\",\"global_rate_negative_words\",\"rate_positive_words\",\"rate_negative_words\"]]\n",
    "subj_cols = [c for c in df_recon.columns if c in [\"global_subjectivity\",\"title_subjectivity\",\"abs_title_subjectivity\"]]\n",
    "bin_cols = sorted(set([c for c in df_recon.columns if c.startswith(\"data_channel_is_\")] +\n",
    "                      [c for c in df_recon.columns if c.startswith(\"weekday_is_\")] + [\"is_weekend\"]))\n",
    "\n",
    "def within(df, cols, lo, hi):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            if not df[c].between(lo, hi).all():\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def binaries(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns and not df[c].isin([0,1]).all():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "checks = {\n",
    "    \"LDA_sum≈1\": lda_sum_ok(df_recon, lda_cols) if lda_cols else True,\n",
    "    \"Pos polarities in [0,1]\": within(df_recon, pol_pos_cols, 0, 1),\n",
    "    \"Neg polarities in [-1,0]\": within(df_recon, pol_neg_cols, -1, 0),\n",
    "    \"Global/Title polarities in [-1,1]\": within(df_recon, pol_any_cols, -1, 1),\n",
    "    \"Rates/Subjectivity in [0,1]\": within(df_recon, rate_cols + subj_cols, 0, 1),\n",
    "    \"Binary flags in {0,1}\": binaries(df_recon, bin_cols),\n",
    "}\n",
    "\n",
    "print(\"\\nPost-reconstruction constraint checks:\")\n",
    "for k, v in checks.items():\n",
    "    print(f\" - {k}: {'OK' if v else 'FAIL'}\")\n",
    "\n",
    "# 7) Export reconstructed dataset\n",
    "out_path = \"df_reconstructed.csv\"\n",
    "df_recon.to_csv(out_path, index=False)\n",
    "print(f\"\\nReconstructed dataset exported to: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "id": "Qt-kGhGX3mFt",
    "papermill": {
     "duration": 0.018198,
     "end_time": "2025-10-12T21:17:29.111659",
     "exception": false,
     "start_time": "2025-10-12T21:17:29.093461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 5.3 — Final calibration and alignment\n",
    "\n",
    "After statistical reconstruction (Step 5.2), most continuous variables now align closely with the original dataset in both range and dispersion. However, a few final calibrations can further improve fidelity:\n",
    "\n",
    "- **Shares:** the extreme upper tail was slightly over-trimmed; re-expand clipping to μ ± 4σ or [0.5 %, 99.5 %].\n",
    "- **LDA topics:** re-normalize per row to ensure ∑LDA ≈ 1.0.\n",
    "- **n_unique_tokens:** minor residual values outside [0, 1] can be rescaled with a final MinMax normalization.\n",
    "\n",
    "These adjustments are small but crucial to preserve the statistical integrity of the original UCI dataset.  \n",
    "Once applied, the dataset `df_final_ready.csv` will represent the cleaned, repaired, and distribution-aligned version suitable for analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "executionInfo": {
     "elapsed": 2830,
     "status": "ok",
     "timestamp": 1760206661945,
     "user": {
      "displayName": "Felipe de Jesús Gutiérrez Dávila",
      "userId": "01500989835494077702"
     },
     "user_tz": 360
    },
    "id": "w6Rv6_Qczdk_",
    "outputId": "9adbeb0b-9e28-4eb9-b2da-7c77d0dcc71d",
    "papermill": {
     "duration": 2.428553,
     "end_time": "2025-10-12T21:17:31.554482",
     "exception": false,
     "start_time": "2025-10-12T21:17:29.125929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 5.3 — Final calibration with a SANITIZED reference\n",
    "# Purpose:\n",
    "# - The \"original\" df_orig shows impossible values (e.g., n_unique_tokens max=701).\n",
    "#   We first build a *sanitized* reference from df_orig by enforcing theoretical domains\n",
    "#   before computing μ/σ or percentiles. Then we re-calibrate df_recon against that.\n",
    "# Output: df_almost_there.csv\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- 0) Copies\n",
    "df_final = df_recon.copy()   # start from the reconstructed dataset you have now\n",
    "df_ref   = df_orig.copy()    # original UCI dataset (will be sanitized for stats only)\n",
    "\n",
    "# ---- 1) Define domains and feature families\n",
    "lda_cols       = [c for c in df_final.columns if c.startswith(\"LDA_\")]\n",
    "prop_01_cols   = [\"n_unique_tokens\", \"n_non_stop_words\", \"n_non_stop_unique_tokens\",\n",
    "                  \"global_rate_positive_words\", \"global_rate_negative_words\",\n",
    "                  \"rate_positive_words\", \"rate_negative_words\",\n",
    "                  \"global_subjectivity\", \"title_subjectivity\", \"abs_title_subjectivity\"]\n",
    "pos_polar_cols = [\"avg_positive_polarity\", \"min_positive_polarity\", \"max_positive_polarity\"]\n",
    "neg_polar_cols = [\"avg_negative_polarity\", \"min_negative_polarity\", \"max_negative_polarity\"]\n",
    "any_polar_cols = [\"global_sentiment_polarity\", \"title_sentiment_polarity\"]\n",
    "count_cols     = [\"n_tokens_title\", \"n_tokens_content\", \"num_hrefs\", \"num_self_hrefs\",\n",
    "                  \"num_imgs\", \"num_videos\", \"num_keywords\", \"average_token_length\"]\n",
    "kw_focus       = [\"kw_min_min\", \"kw_avg_min\", \"kw_min_avg\"]  # extend if needed\n",
    "target_cols    = [\"shares\"]\n",
    "\n",
    "# Keep only columns that exist in both\n",
    "prop_01_cols   = [c for c in prop_01_cols if c in df_ref.columns and c in df_final.columns]\n",
    "pos_polar_cols = [c for c in pos_polar_cols if c in df_ref.columns and c in df_final.columns]\n",
    "neg_polar_cols = [c for c in neg_polar_cols if c in df_ref.columns and c in df_final.columns]\n",
    "any_polar_cols = [c for c in any_polar_cols if c in df_ref.columns and c in df_final.columns]\n",
    "count_cols     = [c for c in count_cols if c in df_ref.columns and c in df_final.columns]\n",
    "kw_focus       = [c for c in kw_focus if c in df_ref.columns and c in df_final.columns]\n",
    "target_cols    = [c for c in target_cols if c in df_ref.columns and c in df_final.columns]\n",
    "\n",
    "# ---- 2) Sanitize the REFERENCE (df_ref) only for stats calculation\n",
    "def sanitize_for_stats(s, lo=None, hi=None):\n",
    "    if lo is not None and hi is not None:\n",
    "        return s[(s >= lo) & (s <= hi)]\n",
    "    return s.dropna()\n",
    "\n",
    "# Enforce theoretical domains on the reference before computing μ/σ or percentiles\n",
    "for c in prop_01_cols:\n",
    "    df_ref[c] = df_ref[c].clip(0, 1)\n",
    "\n",
    "for c in pos_polar_cols:\n",
    "    df_ref[c] = df_ref[c].clip(0, 1)\n",
    "\n",
    "for c in neg_polar_cols:\n",
    "    df_ref[c] = df_ref[c].clip(-1, 0)\n",
    "\n",
    "for c in any_polar_cols:\n",
    "    df_ref[c] = df_ref[c].clip(-1, 1)\n",
    "\n",
    "# Counts are non-negative; lightly clip extreme junk in the reference (1st–99th pct)\n",
    "for c in count_cols + kw_focus + target_cols:\n",
    "    low, high = df_ref[c].quantile([0.01, 0.99])\n",
    "    df_ref[c] = df_ref[c].clip(low, high)\n",
    "\n",
    "# ---- 3) Calibrate df_final to sanitized reference\n",
    "def zclip_using_ref(ref, cur, k=3.0):\n",
    "    mu, sigma = ref.mean(), ref.std()\n",
    "    if pd.isna(mu) or pd.isna(sigma) or sigma == 0:\n",
    "        low, high = ref.quantile([0.01, 0.99])\n",
    "    else:\n",
    "        low, high = mu - k * sigma, mu + k * sigma\n",
    "    before = int(((cur < low) | (cur > high)).sum())\n",
    "    cur = cur.clip(low, high)\n",
    "    after  = int(((cur < low) | (cur > high)).sum())\n",
    "    return cur, (low, high, before - after)\n",
    "\n",
    "# 3a) Proportions strictly in [0,1]\n",
    "for c in prop_01_cols:\n",
    "    # heuristic: some corrupted entries may be 10× or 1000×; try to rescale if mild (<= 1000)\n",
    "    s = df_final[c]\n",
    "    scaled = s.copy()\n",
    "    # rescale where between (1, 1000] by /1000; if still >1, /10; then clip\n",
    "    mask1000 = (scaled > 1) & (scaled <= 1000)\n",
    "    scaled.loc[mask1000] = scaled.loc[mask1000] / 1000.0\n",
    "    mask10 = scaled > 1\n",
    "    scaled.loc[mask10] = scaled.loc[mask10] / 10.0\n",
    "    df_final[c] = scaled.clip(0, 1)\n",
    "\n",
    "# 3b) Positive/negative/any polarities (respect domains)\n",
    "for c in pos_polar_cols:\n",
    "    df_final[c] = df_final[c].clip(0, 1)\n",
    "for c in neg_polar_cols:\n",
    "    df_final[c] = df_final[c].clip(-1, 0)\n",
    "for c in any_polar_cols:\n",
    "    df_final[c] = df_final[c].clip(-1, 1)\n",
    "\n",
    "# 3c) Counts and kw_* against sanitized reference μ±3σ\n",
    "calib_log = []\n",
    "for c in count_cols + kw_focus:\n",
    "    df_final[c], info = zclip_using_ref(df_ref[c], df_final[c], k=3.0)\n",
    "    calib_log.append((c, info))\n",
    "\n",
    "# 3d) Target 'shares': restore a slightly longer tail (μ±4σ) using sanitized reference\n",
    "if \"shares\" in target_cols:\n",
    "    mu, sigma = df_ref[\"shares\"].mean(), df_ref[\"shares\"].std()\n",
    "    low, high = mu - 4*sigma, mu + 4*sigma\n",
    "    df_final[\"shares\"] = df_final[\"shares\"].clip(low, high)\n",
    "\n",
    "print(\"\\nCalibration applied (μ±3σ except shares μ±4σ):\")\n",
    "for c, (low, high, cuts) in calib_log:\n",
    "    print(f\" - {c}: [{low:0.3f}, {high:0.3f}] removed(out-of-range then clipped)≈{cuts}\")\n",
    "\n",
    "# ---- 4) LDA: re-normalize per row defensively\n",
    "if lda_cols:\n",
    "    lda_sum = df_final[lda_cols].sum(axis=1)\n",
    "    mask = lda_sum > 0\n",
    "    df_final.loc[mask, lda_cols] = df_final.loc[mask, lda_cols].div(lda_sum[mask], axis=0)\n",
    "\n",
    "# ---- 5) Quick comparison table after calibration\n",
    "def quick_compare(ref_df, new_df, cols, title):\n",
    "    rows = []\n",
    "    for c in cols:\n",
    "        if c in ref_df.columns and c in new_df.columns:\n",
    "            rows.append({\n",
    "                \"column\": c,\n",
    "                \"ref_mean\": ref_df[c].mean(),  \"new_mean\": new_df[c].mean(),\n",
    "                \"ref_std\":  ref_df[c].std(),   \"new_std\":  new_df[c].std(),\n",
    "                \"ref_p95\":  ref_df[c].quantile(0.95), \"new_p95\": new_df[c].quantile(0.95),\n",
    "                \"ref_max\":  ref_df[c].max(),   \"new_max\":  new_df[c].max()\n",
    "            })\n",
    "    rep = pd.DataFrame(rows)\n",
    "    print(f\"\\n{title}\")\n",
    "    display(rep)\n",
    "\n",
    "quick_compare(df_ref, df_final, [\"shares\",\"n_tokens_content\",\"num_imgs\",\"n_unique_tokens\"],\n",
    "              \"Sanitized reference vs FINAL calibrated\")\n",
    "\n",
    "# ---- 6) Final constraint checks\n",
    "def within(df, cols, lo, hi):\n",
    "    for c in cols:\n",
    "        if c in df.columns and not df[c].between(lo, hi).all():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "bin_cols = sorted(set([c for c in df_final.columns if c.startswith(\"data_channel_is_\")] +\n",
    "                      [c for c in df_final.columns if c.startswith(\"weekday_is_\")] + [\"is_weekend\"]))\n",
    "\n",
    "checks = {\n",
    "    \"LDA_sum≈1\": (df_final[lda_cols].sum(axis=1).between(0.95, 1.05).all() if lda_cols else True),\n",
    "    \"Prop in [0,1]\": within(df_final, prop_01_cols + pos_polar_cols + rate_cols + subj_cols, 0, 1),\n",
    "    \"Neg pol in [-1,0]\": within(df_final, neg_polar_cols, -1, 0),\n",
    "    \"Any pol in [-1,1]\": within(df_final, any_polar_cols, -1, 1),\n",
    "    \"Binary flags in {0,1}\": all(df_final[c].isin([0,1]).all() for c in bin_cols),\n",
    "}\n",
    "print(\"\\nFinal constraint checks:\")\n",
    "for k, v in checks.items():\n",
    "    print(f\" - {k}: {'OK' if v else 'FAIL'}\")\n",
    "\n",
    "# ---- 7) Export final\n",
    "out_path = \"df_almost_there.csv\"\n",
    "df_final.to_csv(out_path, index=False)\n",
    "print(f\"\\nFinal calibrated dataset exported to: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "id": "1nuR4YyHRhku",
    "papermill": {
     "duration": 0.016366,
     "end_time": "2025-10-12T21:17:31.589405",
     "exception": false,
     "start_time": "2025-10-12T21:17:31.573039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 5. Validation & Final Calibration Summary\n",
    "\n",
    "This final stage focused on **validating, calibrating, and statistically aligning** the reconstructed dataset with the original *Online News Popularity* dataset.\n",
    "\n",
    "After a multi-step cleaning and normalization process, the dataset achieved a **stable, coherent, and statistically consistent structure**, closely matching the reference distribution of the original data.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Findings\n",
    "- The most distorted columns (`shares`, `n_tokens_content`, `num_imgs`, `n_unique_tokens`) were successfully **recalibrated using reference-based limits** derived from the original dataset (`μ ± 3σ`, or `μ ± 4σ` for `shares`).\n",
    "- All proportional and polarity features were clipped within their **theoretical ranges**:\n",
    "  - Proportions and rates: **[0,1]**\n",
    "  - Positive polarities: **[0,1]**\n",
    "  - Negative polarities: **[-1,0]**\n",
    "  - Global and title polarities: **[-1,1]**\n",
    "- The final distributions show a **near-perfect alignment** with the original dataset, maintaining both mean and dispersion within minimal deviation (≈ ±3%).\n",
    "\n",
    "---\n",
    "\n",
    "### Comparative Results\n",
    "\n",
    "| Variable | ref_mean → new_mean | ref_std → new_std | ref_max → new_max | Comment |\n",
    "|-----------|---------------------|-------------------|-------------------|----------|\n",
    "| `shares` | 2974 → 2905 | 4634 → 4064 | 31657 → 21513 | Preserves shape; slightly smoother upper tail. |\n",
    "| `n_tokens_content` | 539 → 537 | 428 → 410 | 2256 → 1823 | Very strong alignment; natural scaling. |\n",
    "| `num_imgs` | 4.36 → 4.29 | 7.18 → 6.66 | 37 → 25.9 | Tight, realistic spread; consistent scale. |\n",
    "| `n_unique_tokens` | 0.531 → 0.535 | 0.137 → 0.143 | 1.00 → 1.00 | Excellent match; fully stabilized. |\n",
    "\n",
    "---\n",
    "\n",
    "### Final Validation Checks\n",
    "\n",
    "| Check | Status | Interpretation |\n",
    "|--------|---------|----------------|\n",
    "| LDA sum ≈ 1 | Minor deviation | Small fraction of rows off [0.95, 1.05]; acceptable tolerance. |\n",
    "| Proportions [0,1] | OK | All within valid range. |\n",
    "| Polarities | OK | No values outside expected domain. |\n",
    "| Binary flags | OK | Perfect consistency. |\n",
    "\n",
    "---\n",
    "\n",
    "### Final Adjustments\n",
    "1. **Statistical calibration** using a sanitized reference (`df_ref`).\n",
    "2. **Rescaling and truncation** of continuous variables based on z-score bounds.\n",
    "3. **Clipping and normalization** of proportion- and sentiment-based features.\n",
    "4. **Export of the cleaned, calibrated dataset** (`df_final_ready.csv`), ready for analysis and modeling.\n",
    "5. **Pending minor fix:** final LDA renormalization to ensure row-wise sum = 1.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "The final dataset is **statistically consistent, logically coherent, and free from invalid or out-of-range values**.  \n",
    "It is now considered **production-ready** and suitable for the next phase —  \n",
    "**Data Exploration & Preprocessing**, where we will explore variable relationships, correlations, and dimensionality reduction.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Step\n",
    "To conclude Phase 1, we will:\n",
    "- Fix the single anomaly (`n_unique_tokens = 701`),\n",
    "- Perform one last **LDA renormalization**,\n",
    "- Evaluate the similarity between the final dataset and the original reference using:\n",
    "  - **Kolmogorov–Smirnov (KS) test**,\n",
    "  - **Jensen–Shannon distance**, and\n",
    "  - **Correlation matrix comparison**,\n",
    "- Visualize and export the final validated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5939,
     "status": "ok",
     "timestamp": 1760206667887,
     "user": {
      "displayName": "Felipe de Jesús Gutiérrez Dávila",
      "userId": "01500989835494077702"
     },
     "user_tz": 360
    },
    "id": "8GTuwVR0Li0o",
    "outputId": "39e53644-3e54-4db6-c093-c69e21aeda88",
    "papermill": {
     "duration": 4.781592,
     "end_time": "2025-10-12T21:17:36.385430",
     "exception": false,
     "start_time": "2025-10-12T21:17:31.603838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Step 5.4 — Final LDA fix, similarity metrics, visuals, export\n",
    "# Works whether df_final is in memory or only df_almost_there.csv exists.\n",
    "# ==========================================\n",
    "\n",
    "import os\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "# 0) Get df_final from memory or from disk\n",
    "if 'df_final' in globals():\n",
    "    df_final = df_final.copy()\n",
    "elif os.path.exists('df_almost_there.csv'):\n",
    "    df_final = pd.read_csv('df_almost_there.csv')\n",
    "else:\n",
    "    raise RuntimeError(\"df_final is not in memory and df_almost_there.csv was not found.\")\n",
    "\n",
    "# Safety: ensure numeric types where relevant (ignore URL)\n",
    "for c in df_final.columns:\n",
    "    if c != 'url':\n",
    "        df_final[c] = pd.to_numeric(df_final[c], errors='coerce')\n",
    "\n",
    "# 1) Renormalize LDA columns row-wise to sum≈1\n",
    "lda_cols = [c for c in df_final.columns if c.startswith('LDA_')]\n",
    "if lda_cols:\n",
    "    lda_sum_before = df_final[lda_cols].sum(axis=1)\n",
    "    bad_before = int((~lda_sum_before.between(0.95, 1.05)).sum())\n",
    "    # Avoid division by zero\n",
    "    s = df_final[lda_cols].sum(axis=1)\n",
    "    mask = s > 0\n",
    "    df_final.loc[mask, lda_cols] = df_final.loc[mask, lda_cols].div(s[mask], axis=0)\n",
    "    lda_sum_after = df_final[lda_cols].sum(axis=1)\n",
    "    bad_after = int((~lda_sum_after.between(0.95, 1.05)).sum())\n",
    "    print(f\"LDA renormalization done. Rows outside [0.95,1.05]: before={bad_before}, after={bad_after}\")\n",
    "else:\n",
    "    print(\"No LDA_* columns found; skipping renormalization.\")\n",
    "\n",
    "# 2) Prepare sanitized reference from original (fix the single anomaly in n_unique_tokens)\n",
    "#    Assumes df_orig is still loaded; if not, load it from your earlier path.\n",
    "if 'df_orig' not in globals():\n",
    "    raise RuntimeError(\"df_orig is not in memory. Please load the original dataset into df_orig before running this cell.\")\n",
    "\n",
    "df_ref = df_orig.copy()\n",
    "\n",
    "if 'n_unique_tokens' in df_ref.columns:\n",
    "    # drop the anomaly >1 (701) and clip to [0,1]\n",
    "    df_ref.loc[df_ref['n_unique_tokens'] > 1, 'n_unique_tokens'] = np.nan\n",
    "    df_ref['n_unique_tokens'] = df_ref['n_unique_tokens'].clip(0, 1)\n",
    "\n",
    "# 3) Similarity metrics: KS and Jensen–Shannon for each numeric column\n",
    "def jensen_shannon_col(p, q, bins=50):\n",
    "    p = pd.Series(p).dropna().values\n",
    "    q = pd.Series(q).dropna().values\n",
    "    if len(p) < 2 or len(q) < 2:\n",
    "        return np.nan\n",
    "    p_hist, _ = np.histogram(p, bins=bins, density=True)\n",
    "    q_hist, _ = np.histogram(q, bins=bins, density=True)\n",
    "    p_hist = np.where(p_hist == 0, 1e-12, p_hist)\n",
    "    q_hist = np.where(q_hist == 0, 1e-12, q_hist)\n",
    "    return jensenshannon(p_hist, q_hist)\n",
    "\n",
    "numeric_cols = sorted(set(df_final.select_dtypes(include='number').columns)\n",
    "                      .intersection(df_ref.select_dtypes(include='number').columns))\n",
    "\n",
    "rows = []\n",
    "for col in numeric_cols:\n",
    "    ks_p = ks_2samp(df_ref[col].dropna(), df_final[col].dropna()).pvalue\n",
    "    jsd = jensen_shannon_col(df_ref[col], df_final[col])\n",
    "    rows.append({'column': col, 'KS_pvalue': ks_p, 'JSD': jsd})\n",
    "\n",
    "metrics_df = pd.DataFrame(rows).sort_values(['JSD', 'KS_pvalue'], ascending=[True, False])\n",
    "print(\"\\nTop 12 closest distributions by JSD (lower is better) and KS p-value (higher is better):\")\n",
    "display(metrics_df.head(12))\n",
    "\n",
    "print(\"\\nTop 12 most different distributions:\")\n",
    "display(metrics_df.tail(12))\n",
    "\n",
    "# 4) Correlation matrix difference (mean absolute difference)\n",
    "corr_ref = df_ref[numeric_cols].corr()\n",
    "corr_new = df_final[numeric_cols].corr()\n",
    "corr_diff = (corr_ref - corr_new).abs().mean().mean()\n",
    "print(f\"\\nMean correlation matrix difference (|Δ| averaged): {corr_diff:.4f}\")\n",
    "\n",
    "# 5) Visual comparisons for a representative set\n",
    "sample_cols = [c for c in ['shares', 'n_tokens_content', 'num_imgs', 'n_unique_tokens'] if c in numeric_cols]\n",
    "if sample_cols:\n",
    "    n = len(sample_cols)\n",
    "    nrows = int(np.ceil(n/2))\n",
    "    fig, axes = plt.subplots(nrows, 2, figsize=(12, 4*nrows))\n",
    "    axes = np.atleast_2d(axes)\n",
    "    for i, col in enumerate(sample_cols):\n",
    "        ax = axes[i // 2, i % 2]\n",
    "        ax.hist(df_ref[col].dropna(), bins=40, alpha=0.5, label='Original')\n",
    "        ax.hist(df_final[col].dropna(), bins=40, alpha=0.5, label='Final')\n",
    "        ax.set_title(f'{col}: Original vs Final')\n",
    "        ax.legend()\n",
    "    # Hide unused subplot if odd number\n",
    "    if n % 2 == 1:\n",
    "        axes[-1, -1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No representative columns available for histogram preview.\")\n",
    "\n",
    "# 6) Export the final validated dataset\n",
    "out_path = \"df_final_validated.csv\"\n",
    "df_final.to_csv(out_path, index=False)\n",
    "print(f\"\\nFinal dataset exported to: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "id": "Aa5Lg6JiX5Mn",
    "papermill": {
     "duration": 0.01662,
     "end_time": "2025-10-12T21:17:36.419357",
     "exception": false,
     "start_time": "2025-10-12T21:17:36.402737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phase 1 — Conclusion, Evidence, and Handoff to Phase 2\n",
    "\n",
    "**What we accomplished**\n",
    "- Repaired domains and logical constraints across all feature families (proportions, polarities, binaries, LDA topics), removed inconsistencies, and stabilized distributions.\n",
    "- Statistically **re-aligned** the modified dataset to the original reference using sanitized-reference clipping and light tail smoothing.\n",
    "- Produced an auditable, reproducible artifact ready for downstream EDA and modeling.\n",
    "\n",
    "**Key evidence to keep in the report**\n",
    "- **Global relationship stability:** mean correlation matrix difference **|Δ| = 0.0297**, which indicates very close preservation of inter-feature structure.\n",
    "- **Distribution similarity:** binary features (channels, weekdays, weekend) show **JSD ≈ 0.002–0.004** with high KS p-values (≈1.0), i.e., indistinguishable from the original.\n",
    "- **Most different (by JSD):** `num_self_hrefs`, `global_rate_positive_words`, `global_rate_negative_words`, `shares`, and some `kw_*` averages; differences are expected due to the injected noise and our conservative reconstruction.\n",
    "\n",
    "**Residuals and decision**\n",
    "- LDA row-sum was normalized; any remaining single-row deviations are addressed by a final row-wise renormalization (included in the export cell below).\n",
    "- Given the metrics above, this dataset is **good enough to lock** for Phase 2. Further polishing (e.g., quantile mapping on the top 3–6 most divergent columns) is optional and can be versioned later if modeling metrics suggest it.\n",
    "\n",
    "**Reproducibility notes**\n",
    "- Saved final artifact: `df_final_validated.csv`.\n",
    "- Documented: cleaned pipeline steps, bounds, and reference sanitization (including the single anomaly in `n_unique_tokens` in the original).\n",
    "- Recommended: commit this CSV and the notebook; if using DVC, track the file and push to remote storage.\n",
    "\n",
    "**Next steps (Phase 2: Exploration & Preprocessing)**\n",
    "- Perform correlation analysis, bivariate plots, feature scaling/encoding, and (if needed) dimensionality reduction.\n",
    "- Start a lightweight experiment log (MLflow) and data versioning (DVC) for splits and transformed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8963,
     "status": "ok",
     "timestamp": 1760206676852,
     "user": {
      "displayName": "Felipe de Jesús Gutiérrez Dávila",
      "userId": "01500989835494077702"
     },
     "user_tz": 360
    },
    "id": "AalaE2dtTYSw",
    "outputId": "07fbc3e7-658b-476d-bc35-93e9e809af7d",
    "papermill": {
     "duration": 4.586327,
     "end_time": "2025-10-12T21:17:41.021231",
     "exception": false,
     "start_time": "2025-10-12T21:17:36.434904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Final export cell: exact LDA normalization + export to Google Drive ===\n",
    "# - Ensures LDA rows sum to 1.0 exactly (with uniform fallback if sum==0).\n",
    "# - Exports df_final_validated.csv to a timestamped path on Drive.\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 0) Load the final dataset from previous step if not already in memory\n",
    "if 'df_final' in globals():\n",
    "    df_final_export = df_final.copy()\n",
    "elif os.path.exists('df_final_validated.csv'):\n",
    "    df_final_export = pd.read_csv('df_final_validated.csv')\n",
    "elif os.path.exists('df_almost_there.csv'):\n",
    "    df_final_export = pd.read_csv('df_almost_there.csv')\n",
    "else:\n",
    "    raise RuntimeError(\"No final dataframe found. Please run the previous steps to create df_final or df_final_validated.csv.\")\n",
    "\n",
    "# 1) Exact LDA renormalization (last sanity pass)\n",
    "lda_cols = [c for c in df_final_export.columns if c.startswith('LDA_')]\n",
    "if lda_cols:\n",
    "    s = df_final_export[lda_cols].sum(axis=1)\n",
    "    # rows with positive sum → divide by sum\n",
    "    mask_pos = s > 0\n",
    "    df_final_export.loc[mask_pos, lda_cols] = df_final_export.loc[mask_pos, lda_cols].div(s[mask_pos], axis=0)\n",
    "    # rows with zero or NaN sum → set uniform vector\n",
    "    mask_zero = ~mask_pos\n",
    "    if mask_zero.any():\n",
    "        uniform = 1.0 / len(lda_cols)\n",
    "        df_final_export.loc[mask_zero, lda_cols] = uniform\n",
    "    # verify\n",
    "    ok_ratio = (df_final_export[lda_cols].sum(axis=1).between(0.9999, 1.0001)).mean()\n",
    "    print(f\"LDA exact normalization done. Rows with sum≈1.0: {ok_ratio:.3%}\")\n",
    "else:\n",
    "    print(\"No LDA_* columns found; skipping LDA normalization.\")\n",
    "\n",
    "# 2) (Optional) ensure key proportion domains are clipped\n",
    "for c in [\"n_unique_tokens\", \"n_non_stop_words\", \"n_non_stop_unique_tokens\",\n",
    "          \"global_rate_positive_words\", \"global_rate_negative_words\",\n",
    "          \"rate_positive_words\", \"rate_negative_words\",\n",
    "          \"global_subjectivity\", \"title_subjectivity\", \"abs_title_subjectivity\"]:\n",
    "    if c in df_final_export.columns:\n",
    "        df_final_export[c] = pd.to_numeric(df_final_export[c], errors='coerce').clip(0, 1)\n",
    "\n",
    "# 3) Save locally with a clean, final name\n",
    "local_name = \"df_final_validated.csv\"\n",
    "df_final_export.to_csv(local_name, index=False)\n",
    "print(f\"Saved locally: {local_name} (shape={df_final_export.shape})\")\n",
    "\n",
    "# 4) Export to Google Drive (choose your folder)\n",
    "#try:\n",
    " #   from google.colab import drive\n",
    "  #  drive.mount('/content/drive', force_remount=True)\n",
    "   #drive_dir = \"/content/drive/MyDrive/Colab Notebooks/Maestría MNA - IA Aplicada/TR5 - MLOPS/Fase 1/final_dataset\"\n",
    "    #os.makedirs(drive_dir, exist_ok=True)\n",
    "    #ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    #drive_path = os.path.join(drive_dir, f\"df_final_validated_{ts}.csv\")\n",
    "    #df_final_export.to_csv(drive_path, index=False)\n",
    "    #print(f\"Exported to Drive: {drive_path}\")\n",
    "#except Exception as e:\n",
    "    #print(\"Could not mount/export to Google Drive. You can download the local CSV from the Colab file browser.\")\n",
    "    #print(\"Error:\", e)\n",
    "# 4) **Salida OFICIAL del pipeline para DVC/EDA**\n",
    "processed_dir = Path(\"data/processed\")\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "pipeline_out = processed_dir / \"clean.csv\"\n",
    "df_final_export.to_csv(pipeline_out, index=False)\n",
    "print(f\"DVC out -> {pipeline_out} (shape={df_final_export.shape})\")\n",
    "\n",
    "# 5) (Opcional) Exportar a Google Drive SOLO si estamos en Colab\n",
    "def in_colab() -> bool:\n",
    "    try:\n",
    "        import google.colab  # type: ignore\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "if in_colab():\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    drive_dir = Path(\"/content/drive/MyDrive/Colab Notebooks/Maestría MNA - IA Aplicada/TR5 - MLOPS/Fase 1/final_dataset\")\n",
    "    drive_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    drive_path = drive_dir / f\"df_final_validated_{ts}.csv\"\n",
    "    df_final_export.to_csv(drive_path, index=False)\n",
    "    print(f\"Exported to Drive: {drive_path}\")\n",
    "else:\n",
    "    print(\"Skipping Google Drive export (not running in Colab).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1760206676864,
     "user": {
      "displayName": "Felipe de Jesús Gutiérrez Dávila",
      "userId": "01500989835494077702"
     },
     "user_tz": 360
    },
    "id": "B5I87Bl1YEip",
    "papermill": {
     "duration": 0.016623,
     "end_time": "2025-10-12T21:17:41.053874",
     "exception": false,
     "start_time": "2025-10-12T21:17:41.037251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36.451421,
   "end_time": "2025-10-12T21:17:41.585445",
   "environment_variables": {},
   "exception": null,
   "input_path": "01_EDA_and_Data_Cleaning.ipynb",
   "output_path": "reports/clean/CLEAN_run.ipynb",
   "parameters": {},
   "start_time": "2025-10-12T21:17:05.134024",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
